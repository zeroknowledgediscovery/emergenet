{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eada972",
   "metadata": {},
   "source": [
    "# Enet Predictions vs. WHO Predictions\n",
    "- Compare Enet predictions and WHO predictions\n",
    "- For each season, take the average levenshtein distance between the prediction and the top 10 dominant strains\n",
    "- Truncate sequence to HA 565, NA 468\n",
    "- WHO predictions from [WHO vaccine recommendations](https://www.who.int/teams/global-influenza-programme/vaccines/who-recommendations/recommendations-for-influenza-vaccine-composition-archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ab0dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Levenshtein import distance\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5a547b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WHO_DIR = 'raw_data/who/'\n",
    "PRED_DIR = 'results/enet_predictions/'\n",
    "DOM_DIR = 'results/dominant_sequences/'\n",
    "OUT_DIR = 'results/enet_who_comparison/'\n",
    "\n",
    "FILES = ['north_h1n1', 'north_h3n2', 'south_h1n1', 'south_h3n2']\n",
    "\n",
    "NORTH_YEARS = []\n",
    "for i in np.arange(2, 23):\n",
    "    YEAR = ''\n",
    "    if i < 10:\n",
    "        YEAR += '0' + str(i)\n",
    "    else:\n",
    "        YEAR += (str(i))\n",
    "    if i + 1 < 10:\n",
    "        YEAR += '_0' + str(i + 1)\n",
    "    else:\n",
    "        YEAR += '_' + str(i + 1)\n",
    "    NORTH_YEARS.append(YEAR)\n",
    "        \n",
    "SOUTH_YEARS = []\n",
    "for i in np.arange(3, 24):\n",
    "    if i < 10:\n",
    "        SOUTH_YEARS.append('0' + str(i))\n",
    "    else:\n",
    "        SOUTH_YEARS.append(str(i))\n",
    "\n",
    "NA_TRUNC = 468 # 2 less than official length of 470\n",
    "HA_TRUNC = 565 # 2 less than official length of 567"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f15349",
   "metadata": {},
   "source": [
    "## Construct DataFrame\n",
    "- `season`: 02-03 through 22-23 for north, 03 through 23 for south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e959c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:    \n",
    "    df = pd.read_csv(PRED_DIR + FILE + '_predictions.csv')\n",
    "    \n",
    "    # WHO recommendation name, sequence\n",
    "    who_ha_df = pd.read_csv(WHO_DIR + FILE + '_ha.csv')\n",
    "    who_na_df = pd.read_csv(WHO_DIR + FILE + '_na.csv')\n",
    "    df['name_who'] = who_ha_df['who_recommendation_name']\n",
    "    df['ha_seq_who'] = who_ha_df['who_recommendation_sequence']\n",
    "    df['na_seq_who'] = who_na_df['who_recommendation_sequence']\n",
    "    \n",
    "    # error columns\n",
    "    for i in range(3):\n",
    "        df['ha_enet_error_' + str(i)] = -1 * np.ones(len(df))\n",
    "        df['na_enet_error_' + str(i)] = -1 * np.ones(len(df))\n",
    "    df['ha_who_error'] = -1 * np.ones(len(df))\n",
    "    df['na_who_error'] = -1 * np.ones(len(df))\n",
    "    \n",
    "    df.to_csv(OUT_DIR + FILE + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d1fde",
   "metadata": {},
   "source": [
    "## Compute Enet and WHO Errors\n",
    "- For each season, take the average Levenshtein distance between the prediction and the top 10 dominant strains\n",
    "    - Dominant strains in `results/dominant_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f59d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    seasons = df['season'].values\n",
    "    \n",
    "    for i in range(3):\n",
    "        enet_errors_ha = []\n",
    "        enet_errors_na = []\n",
    "        who_errors_ha = []\n",
    "        who_errors_na = []\n",
    "        for season in seasons:\n",
    "            season_str = str(season)\n",
    "            if len(season_str) == 1:\n",
    "                season_str = str('0' + season_str)\n",
    "            # read dominant sequences\n",
    "            DATA_DIR_HA = DOM_DIR + FILE + '_ha/' + FILE + '_ha_' + season_str + '.csv'\n",
    "            DATA_DIR_NA = DOM_DIR + FILE + '_na/' + FILE + '_na_' + season_str + '.csv'\n",
    "            if not os.path.isfile(DATA_DIR_HA):\n",
    "                enet_errors_ha.append(-1)\n",
    "                enet_errors_na.append(-1)\n",
    "                who_errors_ha.append(-1)\n",
    "                who_errors_na.append(-1)\n",
    "                continue\n",
    "            dom_df_ha = pd.read_csv(DATA_DIR_HA, index_col=0)\n",
    "            dom_df_na = pd.read_csv(DATA_DIR_NA, index_col=0)\n",
    "            top_dominant_seqs_ha = dom_df_ha['sequence'].values[:10]\n",
    "            top_dominant_seqs_na = dom_df_na['sequence'].values[:10]\n",
    "            # access enet and who recommendations\n",
    "            enet_ha_seq = df[df['season'] == season]['ha_seq_' + str(i)].values[0][:HA_TRUNC]\n",
    "            enet_na_seq = df[df['season'] == season]['na_seq_' + str(i)].values[0][:NA_TRUNC]\n",
    "            who_ha_seq = df[df['season'] == season]['ha_seq_who'].values[0][:HA_TRUNC]\n",
    "            who_na_seq = df[df['season'] == season]['na_seq_who'].values[0][:NA_TRUNC]\n",
    "            # find average enet and who errors\n",
    "            total_enet_error_ha = 0\n",
    "            total_enet_error_na = 0\n",
    "            total_who_error_ha = 0\n",
    "            total_who_error_na = 0\n",
    "            for domseq in top_dominant_seqs_ha:\n",
    "                total_enet_error_ha += distance(enet_ha_seq, domseq[:HA_TRUNC])\n",
    "                total_who_error_ha += distance(who_ha_seq, domseq[:HA_TRUNC])\n",
    "            for domseq in top_dominant_seqs_na:\n",
    "                total_enet_error_na += distance(enet_na_seq, domseq[:NA_TRUNC])\n",
    "                total_who_error_na += distance(who_na_seq, domseq[:NA_TRUNC])\n",
    "            enet_errors_ha.append(total_enet_error_ha/10)\n",
    "            enet_errors_na.append(total_enet_error_na/10)\n",
    "            who_errors_ha.append(total_who_error_ha/10)\n",
    "            who_errors_na.append(total_who_error_na/10)\n",
    "\n",
    "        # add to dataframe\n",
    "        df['ha_enet_error_' + str(i)] = enet_errors_ha\n",
    "        df['na_enet_error_' + str(i)] = enet_errors_na\n",
    "        df['ha_who_error'] = who_errors_ha\n",
    "        df['na_who_error'] = who_errors_na\n",
    "    \n",
    "    df['best_ha_enet_error'] = df[['ha_enet_error_0','ha_enet_error_1']].min(axis=1)\n",
    "    df['best_na_enet_error'] = df[['na_enet_error_0','na_enet_error_1']].min(axis=1)\n",
    "    df.to_csv(OUT_DIR + FILE + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b638ae69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>north_season</th>\n",
       "      <th>north_h1n1_ha</th>\n",
       "      <th>north_h1n1_na</th>\n",
       "      <th>north_h3n2_ha</th>\n",
       "      <th>north_h3n2_na</th>\n",
       "      <th>south_season</th>\n",
       "      <th>south_h1n1_ha</th>\n",
       "      <th>south_h1n1_na</th>\n",
       "      <th>south_h3n2_ha</th>\n",
       "      <th>south_h3n2_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02_03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03_04</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>04</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04_05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>05</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05_06</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>06</td>\n",
       "      <td>7.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06_07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>07</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07_08</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>08</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>08_09</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>09_10</td>\n",
       "      <td>118.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10_11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11_12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12_13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13_14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14_15</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15_16</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16_17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17_18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18_19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19_20</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20_21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21_22</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22_23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   north_season  north_h1n1_ha  north_h1n1_na  north_h3n2_ha  north_h3n2_na  \\\n",
       "0         02_03            3.0            1.0           17.0           13.0   \n",
       "1         03_04            4.0           -1.0           20.0           10.0   \n",
       "2         04_05            5.0            1.0            3.0            2.0   \n",
       "3         05_06            4.6            3.0            5.0            0.0   \n",
       "4         06_07            4.0            3.0           -1.0           -2.0   \n",
       "5         07_08           -4.0            2.0            3.0            7.0   \n",
       "6         08_09           -2.0            0.0            3.0            1.0   \n",
       "7         09_10          118.0           89.0            2.0            2.0   \n",
       "8         10_11            4.0            2.0            5.0            2.0   \n",
       "9         11_12            7.0            4.0            1.0            1.0   \n",
       "10        12_13            5.0            1.0           -1.0            2.0   \n",
       "11        13_14           10.0            3.0            6.0            2.0   \n",
       "12        14_15           10.0            7.0            6.0            1.0   \n",
       "13        15_16           11.0            8.0           10.0            1.0   \n",
       "14        16_17           15.0           14.0           -1.0            4.0   \n",
       "15        17_18            2.0           -1.0            2.0            7.0   \n",
       "16        18_19            5.0            3.0           -6.0           -1.1   \n",
       "17        19_20            7.0            6.0            3.0            7.0   \n",
       "18        20_21            5.0            0.0           -1.6           -2.0   \n",
       "19        21_22           -2.0          -13.0            0.0            0.0   \n",
       "20        22_23            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   south_season  south_h1n1_ha  south_h1n1_na  south_h3n2_ha  south_h3n2_na  \n",
       "0            03            3.0            1.0           23.0            9.0  \n",
       "1            04            4.3            1.0            3.0            1.0  \n",
       "2            05            5.4            2.0            0.0            2.0  \n",
       "3            06            7.4            5.0            5.8            0.5  \n",
       "4            07            8.0            2.0            4.0            6.0  \n",
       "5            08            7.0           24.0            1.0           -1.0  \n",
       "6            09            2.0            0.0            2.0            0.0  \n",
       "7            10            1.0            2.0            3.0            0.0  \n",
       "8            11            3.0            1.0           -2.0            3.0  \n",
       "9            12            1.8            1.0            4.0            5.0  \n",
       "10           13            4.0            0.0            3.0            2.0  \n",
       "11           14            9.0            7.0            3.0           -1.0  \n",
       "12           15            8.0            6.0           10.0            3.0  \n",
       "13           16            9.0            9.0            3.0            8.0  \n",
       "14           17            2.0           -1.0            0.0            6.6  \n",
       "15           18            4.0            3.0            2.0            2.0  \n",
       "16           19            4.0            4.0            2.0            3.0  \n",
       "17           20            6.3            6.0           -2.0           -3.0  \n",
       "18           21            5.0           -5.0           18.0            3.0  \n",
       "19           22            4.0            0.0            3.0            2.0  \n",
       "20           23            0.0            0.0            0.0            0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north_errors_df = pd.DataFrame({'north_season':NORTH_YEARS})\n",
    "for FILE in FILES[:2]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    errors_ha = []\n",
    "    errors_na = []\n",
    "    # take minimum error of two largest clusters\n",
    "    for i in range(len(df)):\n",
    "        errors_ha.append(df['ha_who_error'][i] - df['best_ha_enet_error'][i])\n",
    "        errors_na.append(df['na_who_error'][i] - df['best_na_enet_error'][i])\n",
    "    north_errors_df[FILE + '_ha'] = errors_ha\n",
    "    north_errors_df[FILE + '_na'] = errors_na\n",
    "    \n",
    "south_errors_df = pd.DataFrame({'south_season':SOUTH_YEARS})\n",
    "for FILE in FILES[2:]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    errors_ha = []\n",
    "    errors_na = []\n",
    "    # take minimum error of two largest clusters\n",
    "    for i in range(len(df)):\n",
    "        errors_ha.append(df['ha_who_error'][i] - df['best_ha_enet_error'][i])\n",
    "        errors_na.append(df['na_who_error'][i] - df['best_na_enet_error'][i])\n",
    "    south_errors_df[FILE + '_ha'] = errors_ha\n",
    "    south_errors_df[FILE + '_na'] = errors_na\n",
    "    \n",
    "errors_df = north_errors_df.join(south_errors_df, how='outer')\n",
    "errors_df.to_csv(OUT_DIR + 'errors_difference.csv', index=False)\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96aba3",
   "metadata": {},
   "source": [
    "## What if we used a random strain from that season instead of our predicted strain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1dc5d66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for FILE in FILES:    \n",
    "    df = pd.read_csv(PRED_DIR + FILE + '_predictions.csv')[['season']]\n",
    "    # WHO recommendation name, sequence\n",
    "    who_ha_df = pd.read_csv(WHO_DIR + FILE + '_ha.csv')\n",
    "    df['name_who'] = who_ha_df['who_recommendation_name']\n",
    "    df['ha_seq_who'] = who_ha_df['who_recommendation_sequence']\n",
    "    # error columns\n",
    "    df['ha_random_error'] = -1 * np.ones(len(df))\n",
    "    df['ha_who_error'] = -1 * np.ones(len(df))\n",
    "    df.to_csv(OUT_DIR + FILE + '_random.csv', index=False)\n",
    "    \n",
    "NORTH_YEARS_RANDOM = []\n",
    "for i in np.arange(1, 22):\n",
    "    YEAR = ''\n",
    "    if i < 10:\n",
    "        YEAR += '0' + str(i)\n",
    "    else:\n",
    "        YEAR += (str(i))\n",
    "    if i + 1 < 10:\n",
    "        YEAR += '_0' + str(i + 1)\n",
    "    else:\n",
    "        YEAR += '_' + str(i + 1)\n",
    "    NORTH_YEARS_RANDOM.append(YEAR)\n",
    "        \n",
    "SOUTH_YEARS_RANDOM = []\n",
    "for i in np.arange(2, 23):\n",
    "    if i < 10:\n",
    "        SOUTH_YEARS_RANDOM.append('0' + str(i))\n",
    "    else:\n",
    "        SOUTH_YEARS_RANDOM.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63daa0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for FILE in FILES:\n",
    "    YEARS = NORTH_YEARS_RANDOM\n",
    "    if FILE[:5] == 'south':\n",
    "        YEARS = SOUTH_YEARS_RANDOM\n",
    "    \n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    seasons = df['season'].values\n",
    "        \n",
    "    for n in np.arange(30):\n",
    "        random_seqs = []\n",
    "        for YEAR in YEARS:\n",
    "            population_df = pd.read_csv('raw_data/merged/' + FILE + '/' + FILE + '_ha_' + YEAR + '.csv')\n",
    "            random_seq = population_df.sample(1, random_state=n)['sequence'].values[0]\n",
    "            random_seqs.append(random_seq)\n",
    "\n",
    "        random_errors_ha = []\n",
    "        who_errors_ha = []\n",
    "\n",
    "        for i in range(len(seasons)):\n",
    "            season_str = str(seasons[i])\n",
    "            if len(season_str) == 1:\n",
    "                season_str = str('0' + season_str)\n",
    "            # read dominant sequences\n",
    "            DATA_DIR_HA = DOM_DIR + FILE + '_ha/' + FILE + '_ha_' + season_str + '.csv'\n",
    "            if not os.path.isfile(DATA_DIR_HA):\n",
    "                random_errors_ha.append(-1)\n",
    "                who_errors_ha.append(-1)\n",
    "                continue\n",
    "            dom_df_ha = pd.read_csv(DATA_DIR_HA)\n",
    "            top_dominant_seqs_ha = dom_df_ha['sequence'].values[:10]\n",
    "\n",
    "            # access who recommendations\n",
    "            who_ha_seq = df[df['season'] == seasons[i]]['ha_seq_who'].values[0][:HA_TRUNC]\n",
    "\n",
    "            # random seq\n",
    "            random_seq_ha = random_seqs[i][:HA_TRUNC]\n",
    "\n",
    "            # find average enet and who errors\n",
    "            total_random_error_ha = 0\n",
    "            total_who_error_ha = 0\n",
    "            for domseq in top_dominant_seqs_ha:\n",
    "                total_random_error_ha += distance(random_seq_ha, domseq[:HA_TRUNC])\n",
    "                total_who_error_ha += distance(who_ha_seq, domseq[:HA_TRUNC])\n",
    "            random_errors_ha.append(total_random_error_ha/10)\n",
    "            who_errors_ha.append(total_who_error_ha/10)\n",
    "\n",
    "        # add to dataframe\n",
    "        df['ha_random_error_' + str(n)] = random_errors_ha\n",
    "        df['ha_who_error'] = who_errors_ha\n",
    "    \n",
    "    total_random_error = np.zeros(len(df))\n",
    "    for n in np.arange(30):\n",
    "        total_random_error += df['ha_random_error_' + str(n)]\n",
    "    df['avg_random_error'] = total_random_error/30\n",
    "    df.to_csv(OUT_DIR + FILE + '_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e5884c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north_h1n1\n",
      "    Avg. Improvement 5.58\n",
      "    % Improvement: 134.71\n",
      "north_h3n2\n",
      "    Avg. Improvement 2.55\n",
      "    % Improvement: 40.73\n",
      "south_h1n1\n",
      "    Avg. Improvement 3.61\n",
      "    % Improvement: 35.68\n",
      "south_h3n2\n",
      "    Avg. Improvement 3.72\n",
      "    % Improvement: 79.31\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    df1 = pd.read_csv(OUT_DIR + FILES[i] + '_random.csv')[['season','avg_random_error']]\n",
    "    df2 = pd.read_csv(OUT_DIR + FILES[i] + '.csv')[['season','ha_who_error','best_ha_enet_error']]\n",
    "    df1 = df1.merge(df2, on='season')\n",
    "    df1['diff'] = df1['avg_random_error'] - df1['best_ha_enet_error']\n",
    "    print(FILES[i])\n",
    "    print('    Avg. Improvement', round(sum(df1['diff'][:20])/20, 2))\n",
    "    print('    % Improvement:', round(100 * (sum(df1['avg_random_error'][:20])/sum(df1['best_ha_enet_error'][:20])-1), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b44f9-ac29-462c-b5b1-9f63c95920cf",
   "metadata": {},
   "source": [
    "## Two Random Strains per Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "654f5c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m=10\n",
    "for FILE in FILES:\n",
    "    YEARS = NORTH_YEARS_RANDOM\n",
    "    if FILE[:5] == 'south':\n",
    "        YEARS = SOUTH_YEARS_RANDOM\n",
    "    \n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    seasons = df['season'].values\n",
    "        \n",
    "    for n in np.arange(m):\n",
    "        random_seqs = []\n",
    "        for YEAR in YEARS:\n",
    "            population_df = pd.read_csv('raw_data/merged/' + FILE + '/' + FILE + '_ha_' + YEAR + '.csv')\n",
    "            random_seq = population_df.sample(2, random_state=n)['sequence'].values\n",
    "            random_seqs.append(random_seq)\n",
    "\n",
    "        random_errors_ha = []\n",
    "        who_errors_ha = []\n",
    "\n",
    "        for i in range(len(seasons)):\n",
    "            season_str = str(seasons[i])\n",
    "            if len(season_str) == 1:\n",
    "                season_str = str('0' + season_str)\n",
    "            # read dominant sequences\n",
    "            DATA_DIR_HA = DOM_DIR + FILE + '_ha/' + FILE + '_ha_' + season_str + '.csv'\n",
    "            if not os.path.isfile(DATA_DIR_HA):\n",
    "                random_errors_ha.append(-1)\n",
    "                who_errors_ha.append(-1)\n",
    "                continue\n",
    "            dom_df_ha = pd.read_csv(DATA_DIR_HA)\n",
    "            top_dominant_seqs_ha = dom_df_ha['sequence'].values[:10]\n",
    "\n",
    "            # access who recommendations\n",
    "            who_ha_seq = df[df['season'] == seasons[i]]['ha_seq_who'].values[0][:HA_TRUNC]\n",
    "\n",
    "            # random seq\n",
    "            random_seq_ha = random_seqs[i][:HA_TRUNC]\n",
    "\n",
    "            # find average enet and who errors\n",
    "            total_random_error_ha = 0\n",
    "            total_who_error_ha = 0\n",
    "            for domseq in top_dominant_seqs_ha:\n",
    "                total_random_error_ha += min(distance(random_seq_ha[0][:HA_TRUNC], domseq[:HA_TRUNC]), distance(random_seq_ha[1][:HA_TRUNC], domseq[:HA_TRUNC]))\n",
    "                total_who_error_ha += distance(who_ha_seq, domseq[:HA_TRUNC])\n",
    "            random_errors_ha.append(total_random_error_ha/10)\n",
    "            who_errors_ha.append(total_who_error_ha/10)\n",
    "\n",
    "        # add to dataframe\n",
    "        df['ha_random_error_' + str(n)] = random_errors_ha\n",
    "        df['ha_who_error'] = who_errors_ha\n",
    "    \n",
    "    total_random_error = np.zeros(len(df))\n",
    "    for n in np.arange(m):\n",
    "        total_random_error += df['ha_random_error_' + str(n)]\n",
    "    df['avg_random_error'] = total_random_error/m\n",
    "    df.to_csv(OUT_DIR + FILE + '_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f8e1186-1a88-4a52-af68-fb340b8cccea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north_h1n1\n",
      "    Avg. Improvement 2.29\n",
      "    Var. Improvement 10.69\n",
      "    % Improvement: 55.28\n",
      "north_h3n2\n",
      "    Avg. Improvement 0.08\n",
      "    Var. Improvement 3.96\n",
      "    % Improvement: 1.22\n",
      "south_h1n1\n",
      "    Avg. Improvement 1.5\n",
      "    Var. Improvement 5.84\n",
      "    % Improvement: 14.84\n",
      "south_h3n2\n",
      "    Avg. Improvement 0.7\n",
      "    Var. Improvement 4.13\n",
      "    % Improvement: 14.89\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    df1 = pd.read_csv(OUT_DIR + FILES[i] + '_random.csv')[['season','avg_random_error']]\n",
    "    df2 = pd.read_csv(OUT_DIR + FILES[i] + '.csv')[['season','ha_who_error','best_ha_enet_error']]\n",
    "    df1 = df1.merge(df2, on='season')\n",
    "    df1['diff'] = df1['avg_random_error'] - df1['best_ha_enet_error']\n",
    "    print(FILES[i])\n",
    "    print('    Avg. Improvement', round(sum(df1['diff'][:20])/20, 2))\n",
    "    print('    Var. Improvement', round(np.std(df1['diff'][:20]), 2))\n",
    "    print('    % Improvement:', round(100 * (sum(df1['avg_random_error'][:20])/sum(df1['best_ha_enet_error'][:20])-1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7730343-4912-4ae3-831f-eaa3096c519c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
