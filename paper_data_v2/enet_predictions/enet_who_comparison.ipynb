{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eada972",
   "metadata": {},
   "source": [
    "# Enet Predictions vs. WHO Predictions\n",
    "- Compare Enet predictions and WHO predictions\n",
    "- For each season, take the average Levenshtein distance between the predictions and dominant strains\n",
    "- Truncate sequence to HA 565, NA 468\n",
    "- WHO predictions from [WHO vaccine recommendations](https://www.who.int/teams/global-influenza-programme/vaccines/who-recommendations/recommendations-for-influenza-vaccine-composition-archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ab0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Levenshtein import distance\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5a547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHO_DIR = 'raw_data/who/'\n",
    "PRED_DIR = 'results/enet_predictions/'\n",
    "DOM_DIR = 'dominant_sequences/results/'\n",
    "OUT_DIR = 'results/enet_who_comparison/'\n",
    "\n",
    "FILES = ['north_h1n1', 'north_h3n2', 'south_h1n1', 'south_h3n2']\n",
    "\n",
    "NORTH_YEARS = []\n",
    "for i in np.arange(3, 24):\n",
    "    YEAR = ''\n",
    "    if i < 10:\n",
    "        YEAR += '0' + str(i)\n",
    "    else:\n",
    "        YEAR += (str(i))\n",
    "    if i + 1 < 10:\n",
    "        YEAR += '_0' + str(i + 1)\n",
    "    else:\n",
    "        YEAR += '_' + str(i + 1)\n",
    "    NORTH_YEARS.append(YEAR)\n",
    "        \n",
    "SOUTH_YEARS = []\n",
    "for i in np.arange(3, 24):\n",
    "    if i < 10:\n",
    "        SOUTH_YEARS.append('0' + str(i))\n",
    "    else:\n",
    "        SOUTH_YEARS.append(str(i))\n",
    "\n",
    "NA_TRUNC = 468 # 2 less than official length of 470\n",
    "HA_TRUNC = 565 # 2 less than official length of 567"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f15349",
   "metadata": {},
   "source": [
    "## Construct DataFrame\n",
    "- `season`: 03-04 through 23-24 for north, 03 through 23 for south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e959c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:    \n",
    "    df = pd.read_csv(PRED_DIR + FILE + '_predictions.csv')\n",
    "    # who recommendation name, sequence\n",
    "    who_ha_df = pd.read_csv(WHO_DIR + FILE + '.csv')\n",
    "    who_na_df = pd.read_csv(WHO_DIR + FILE + '.csv')\n",
    "    df['name_who'] = who_ha_df['who_recommendation_name']\n",
    "    df['ha_seq_who'] = who_ha_df['ha_sequence']\n",
    "    df['na_seq_who'] = who_na_df['na_sequence']\n",
    "    df.to_csv(OUT_DIR + FILE + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d1fde",
   "metadata": {},
   "source": [
    "## Compute Enet and WHO Errors\n",
    "- For each season, take the best weighted average Levenshtein distance between the two predictions and the dominant strains\n",
    "    - Our predictions are the Emergenet predictions from the two largest clusters\n",
    "    - Dominant strains in `./dominant_sequences/results/`, recall that we clustered the population space under the Levenshtein distance, and found the Levenshtein centroid for each cluster (see `./dominant_sequences/dominant_sequences.ipynb`)\n",
    "    - For each season:\n",
    "        - For each dominant sequence, take the smallest Levenshtein distance between it and our two predictions\n",
    "        - Multiply this by the cluster size the dominant sequence is from, and add it to the total\n",
    "        - Divide the total by the total population size (sum of all cluster sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f59d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    seasons = df['season'].values\n",
    "    \n",
    "    enet_errors_ha = []\n",
    "    enet_errors_na = []\n",
    "    who_errors_ha = []\n",
    "    who_errors_na = []\n",
    "    \n",
    "    for season in seasons:\n",
    "        season_str = str(season)\n",
    "        if len(season_str) == 1:\n",
    "            season_str = str('0' + season_str)\n",
    "            \n",
    "        # read dominant sequences\n",
    "        DATA_DIR_HA = DOM_DIR + FILE + '_ha/' + FILE + '_ha_' + season_str + '/dom_seqs.csv'\n",
    "        DATA_DIR_NA = DOM_DIR + FILE + '_na/' + FILE + '_na_' + season_str + '/dom_seqs.csv'\n",
    "        if not os.path.isfile(DATA_DIR_HA):\n",
    "            enet_errors_ha.append(-1)\n",
    "            enet_errors_na.append(-1)\n",
    "            who_errors_ha.append(-1)\n",
    "            who_errors_na.append(-1)\n",
    "            continue\n",
    "        dom_df_ha = pd.read_csv(DATA_DIR_HA)\n",
    "        dom_df_na = pd.read_csv(DATA_DIR_NA)\n",
    "        \n",
    "        # access enet and who recommendations\n",
    "        enet_ha_seq_0 = df[df['season'] == season]['ha_seq_0'].values[0]\n",
    "        enet_na_seq_0 = df[df['season'] == season]['na_seq_0'].values[0]\n",
    "        enet_ha_seq_1 = df[df['season'] == season]['ha_seq_1'].values[0]\n",
    "        enet_na_seq_1 = df[df['season'] == season]['na_seq_1'].values[0]\n",
    "\n",
    "        if enet_ha_seq_0 == '-1' or enet_ha_seq_1 == '-1':\n",
    "            enet_errors_ha.append(-1)\n",
    "            enet_errors_na.append(-1)\n",
    "            who_errors_ha.append(-1)\n",
    "            who_errors_na.append(-1)\n",
    "            continue\n",
    "\n",
    "        who_ha_seq = df[df['season'] == season]['ha_seq_who'].values[0][:HA_TRUNC]\n",
    "        who_na_seq = df[df['season'] == season]['na_seq_who'].values[0][:NA_TRUNC]\n",
    "        \n",
    "        # find average enet and who errors\n",
    "        total_enet_error_ha = 0\n",
    "        total_enet_error_na = 0\n",
    "        total_who_error_ha = 0\n",
    "        total_who_error_na = 0\n",
    "        \n",
    "        for i in range(len(dom_df_ha)):\n",
    "            domseq = dom_df_ha['sequence'].values[i]\n",
    "            cluster_size = dom_df_ha['cluster_size'].values[i]\n",
    "            total_enet_error_ha += min(distance(enet_ha_seq_0, domseq), distance(enet_ha_seq_1, domseq)) * cluster_size\n",
    "            total_who_error_ha += distance(who_ha_seq, domseq) * cluster_size\n",
    "        for i in range(len(dom_df_na)):\n",
    "            domseq = dom_df_na['sequence'].values[i]\n",
    "            cluster_size = dom_df_na['cluster_size'].values[i]\n",
    "            total_enet_error_na += min(distance(enet_na_seq_0, domseq), distance(enet_na_seq_1, domseq)) * cluster_size\n",
    "            total_who_error_na += distance(who_na_seq, domseq) * cluster_size\n",
    "        enet_errors_ha.append(total_enet_error_ha/sum(dom_df_ha['cluster_size']))\n",
    "        enet_errors_na.append(total_enet_error_na/sum(dom_df_na['cluster_size']))\n",
    "        who_errors_ha.append(total_who_error_ha/sum(dom_df_ha['cluster_size']))\n",
    "        who_errors_na.append(total_who_error_na/sum(dom_df_na['cluster_size']))\n",
    "\n",
    "    # add to dataframe\n",
    "    df['ha_who_error'] = who_errors_ha\n",
    "    df['na_who_error'] = who_errors_na\n",
    "    df['ha_enet_error'] = enet_errors_ha\n",
    "    df['na_enet_error'] = enet_errors_na\n",
    "    df.to_csv(OUT_DIR + FILE + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6486f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat for plotdata in paper\n",
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    df = df.rename(columns={'ha_who_error':'ldistance_WHO', 'ha_enet_error':'ldistance_Qnet_recommendation'})\n",
    "    df.to_csv(OUT_DIR+ 'plotdata/' + FILE + '_ha.csv', index=False)\n",
    "    \n",
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    df = df.rename(columns={'na_who_error':'ldistance_WHO', 'na_enet_error':'ldistance_Qnet_recommendation'})\n",
    "    df.to_csv(OUT_DIR + 'plotdata/' + FILE + '_na.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eba0e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>north_season</th>\n",
       "      <th>north_h1n1_ha</th>\n",
       "      <th>north_h1n1_na</th>\n",
       "      <th>north_h3n2_ha</th>\n",
       "      <th>north_h3n2_na</th>\n",
       "      <th>south_season</th>\n",
       "      <th>south_h1n1_ha</th>\n",
       "      <th>south_h1n1_na</th>\n",
       "      <th>south_h3n2_ha</th>\n",
       "      <th>south_h3n2_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03_04</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>19.63</td>\n",
       "      <td>10.24</td>\n",
       "      <td>03</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>17.23</td>\n",
       "      <td>10.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04_05</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.28</td>\n",
       "      <td>2.50</td>\n",
       "      <td>04</td>\n",
       "      <td>6.83</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05_06</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.79</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>05</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06_07</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>06</td>\n",
       "      <td>5.80</td>\n",
       "      <td>2.44</td>\n",
       "      <td>10.57</td>\n",
       "      <td>2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07_08</td>\n",
       "      <td>6.24</td>\n",
       "      <td>8.84</td>\n",
       "      <td>3.77</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>07</td>\n",
       "      <td>4.32</td>\n",
       "      <td>0.61</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08_09</td>\n",
       "      <td>1.02</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>08</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>14.66</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>09_10</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.57</td>\n",
       "      <td>09</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10_11</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11_12</td>\n",
       "      <td>3.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>1.84</td>\n",
       "      <td>11</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12_13</td>\n",
       "      <td>6.98</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>12</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.05</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13_14</td>\n",
       "      <td>9.34</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.02</td>\n",
       "      <td>13</td>\n",
       "      <td>6.87</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14_15</td>\n",
       "      <td>11.27</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.44</td>\n",
       "      <td>1.50</td>\n",
       "      <td>14</td>\n",
       "      <td>9.03</td>\n",
       "      <td>4.91</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15_16</td>\n",
       "      <td>9.96</td>\n",
       "      <td>6.78</td>\n",
       "      <td>7.37</td>\n",
       "      <td>2.33</td>\n",
       "      <td>15</td>\n",
       "      <td>10.45</td>\n",
       "      <td>8.67</td>\n",
       "      <td>7.38</td>\n",
       "      <td>-3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16_17</td>\n",
       "      <td>14.45</td>\n",
       "      <td>12.66</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>2.59</td>\n",
       "      <td>16</td>\n",
       "      <td>9.97</td>\n",
       "      <td>4.86</td>\n",
       "      <td>1.23</td>\n",
       "      <td>5.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17_18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7.44</td>\n",
       "      <td>17</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>6.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18_19</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>18</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19_20</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.50</td>\n",
       "      <td>19</td>\n",
       "      <td>4.02</td>\n",
       "      <td>2.97</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20_21</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>3.29</td>\n",
       "      <td>20</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.43</td>\n",
       "      <td>6.65</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21_22</td>\n",
       "      <td>5.10</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.28</td>\n",
       "      <td>21</td>\n",
       "      <td>5.22</td>\n",
       "      <td>1.14</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22_23</td>\n",
       "      <td>6.01</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.20</td>\n",
       "      <td>22</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23_24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   north_season  north_h1n1_ha  north_h1n1_na  north_h3n2_ha  north_h3n2_na  \\\n",
       "0         03_04           4.37          -1.00          19.63          10.24   \n",
       "1         04_05           1.62           0.00           5.28           2.50   \n",
       "2         05_06           2.00           0.00           7.79          -1.72   \n",
       "3         06_07           3.49           1.99           1.99           1.00   \n",
       "4         07_08           6.24           8.84           3.77          -1.00   \n",
       "5         08_09           1.02          -0.95           1.00           0.06   \n",
       "6         09_10           1.06           1.06           1.00           1.57   \n",
       "7         10_11           2.30           2.24           0.04           0.00   \n",
       "8         11_12           3.99           1.00           5.99           1.84   \n",
       "9         12_13           6.98           3.31           1.40           0.48   \n",
       "10        13_14           9.34           4.88           4.00           3.02   \n",
       "11        14_15          11.27           6.61           6.44           1.50   \n",
       "12        15_16           9.96           6.78           7.37           2.33   \n",
       "13        16_17          14.45          12.66          -0.22           2.59   \n",
       "14        17_18           1.00           0.00           0.25           7.44   \n",
       "15        18_19           4.00           3.98           2.22          -1.00   \n",
       "16        19_20           2.00          -1.00           1.07           1.50   \n",
       "17        20_21          -0.64           0.10          -2.06           3.29   \n",
       "18        21_22           5.10          -0.36           3.38           0.28   \n",
       "19        22_23           6.01          -0.19           1.08           0.20   \n",
       "20        23_24           0.00           0.00           0.00           0.00   \n",
       "\n",
       "   south_season  south_h1n1_ha  south_h1n1_na  south_h3n2_ha  south_h3n2_na  \n",
       "0            03           4.61          -0.88          17.23          10.50  \n",
       "1            04           6.83           2.70           2.08           1.12  \n",
       "2            05           2.00          -2.08           0.83           1.17  \n",
       "3            06           5.80           2.44          10.57           2.99  \n",
       "4            07           4.32           0.61           5.63           0.58  \n",
       "5            08          -0.43          14.66           1.03           0.03  \n",
       "6            09           1.00          -0.17           1.00           1.98  \n",
       "7            10           4.05           2.86           0.00           0.00  \n",
       "8            11           2.99           1.99           2.79           0.00  \n",
       "9            12           5.64           1.99           3.05           2.71  \n",
       "10           13           6.87           2.13          -1.20           2.19  \n",
       "11           14           9.03           4.91           3.17           0.29  \n",
       "12           15          10.45           8.67           7.38          -3.99  \n",
       "13           16           9.97           4.86           1.23           5.55  \n",
       "14           17           1.02           0.00           0.39           6.37  \n",
       "15           18           3.98           0.98           2.69           0.98  \n",
       "16           19           4.02           2.97           4.23           2.67  \n",
       "17           20           6.12           4.43           6.65           1.82  \n",
       "18           21           5.22           1.14           3.70           0.70  \n",
       "19           22           5.99           0.08           1.28           0.40  \n",
       "20           23           0.00           0.00           0.00           0.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north_errors_df = pd.DataFrame({'north_season':NORTH_YEARS})\n",
    "for FILE in FILES[:2]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    north_errors_df[FILE + '_ha'] = df['ha_who_error'] - df['ha_enet_error']\n",
    "    north_errors_df[FILE + '_na'] = df['na_who_error'] - df['na_enet_error']\n",
    "    \n",
    "south_errors_df = pd.DataFrame({'south_season':SOUTH_YEARS})\n",
    "for FILE in FILES[2:]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    south_errors_df[FILE + '_ha'] = df['ha_who_error'] - df['ha_enet_error']\n",
    "    south_errors_df[FILE + '_na'] = df['na_who_error'] - df['na_enet_error']\n",
    "    \n",
    "errors_df = north_errors_df.join(south_errors_df, how='outer')\n",
    "errors_df.to_csv(OUT_DIR + 'errors_difference.csv', index=False)\n",
    "errors_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96aba3",
   "metadata": {},
   "source": [
    "## What if we used a random strain from that season instead of our predicted strain?\n",
    "- Select two random strains from each season to be our \"predictions\"\n",
    "- Perform the same analysis as before with these random strains\n",
    "- Repeat n times and take the 95th percentile of errors among these n repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1181083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:    \n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    df.to_csv(OUT_DIR + FILE + '_random.csv', index=False)\n",
    "    \n",
    "NORTH_YEARS_RANDOM = []\n",
    "for i in np.arange(2, 23):\n",
    "    YEAR = ''\n",
    "    if i < 10:\n",
    "        YEAR += '0' + str(i)\n",
    "    else:\n",
    "        YEAR += (str(i))\n",
    "    if i + 1 < 10:\n",
    "        YEAR += '_0' + str(i + 1)\n",
    "    else:\n",
    "        YEAR += '_' + str(i + 1)\n",
    "    NORTH_YEARS_RANDOM.append(YEAR)\n",
    "        \n",
    "SOUTH_YEARS_RANDOM = []\n",
    "for i in np.arange(2, 23):\n",
    "    if i < 10:\n",
    "        SOUTH_YEARS_RANDOM.append('0' + str(i))\n",
    "    else:\n",
    "        SOUTH_YEARS_RANDOM.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63daa0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:\n",
    "    YEARS = NORTH_YEARS_RANDOM\n",
    "    if FILE[:5] == 'south':\n",
    "        YEARS = SOUTH_YEARS_RANDOM\n",
    "    \n",
    "    random_errors_ha = [[] for x in range(21)]\n",
    "    random_errors_na = [[] for x in range(21)]\n",
    "    \n",
    "    n = 20\n",
    "    for state in range(n):\n",
    "        random_seqs = []\n",
    "        for YEAR in YEARS:\n",
    "            population_df = pd.read_csv('raw_data/merged/' + FILE + '/' + FILE + '_' + YEAR + '.csv')\n",
    "            population_df = population_df.drop_duplicates(subset=['sequence'])\n",
    "            random_seqs.append(population_df.sample(2, random_state=state))\n",
    "\n",
    "        df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "        seasons = df['season'].values  \n",
    "        for i in range(len(seasons)):\n",
    "            season_str = str(seasons[i])\n",
    "            if len(season_str) == 1:\n",
    "                season_str = str('0' + season_str)\n",
    "            # read dominant sequences\n",
    "            DATA_DIR_HA = DOM_DIR + FILE + '_ha/' + FILE + '_ha_' + season_str + '/dom_seqs.csv'\n",
    "            DATA_DIR_NA = DOM_DIR + FILE + '_na/' + FILE + '_na_' + season_str + '/dom_seqs.csv'\n",
    "            if not os.path.isfile(DATA_DIR_HA):\n",
    "                random_errors_ha[i] = -1\n",
    "                random_errors_na[i] = -1\n",
    "                continue\n",
    "            dom_df_ha = pd.read_csv(DATA_DIR_HA)\n",
    "            dom_df_na = pd.read_csv(DATA_DIR_NA)\n",
    "\n",
    "            # random seq\n",
    "            random_seq_ha_0 = random_seqs[i]['sequence'].values[0]\n",
    "            random_seq_na_0 = random_seqs[i]['sequence_na'].values[0]\n",
    "            random_seq_ha_1 = random_seqs[i]['sequence'].values[1]\n",
    "            random_seq_na_1 = random_seqs[i]['sequence_na'].values[1]\n",
    "\n",
    "            # find average random errors\n",
    "            total_random_error_ha = 0\n",
    "            total_random_error_na = 0\n",
    "            for j in range(len(dom_df_ha)):\n",
    "                domseq = dom_df_ha['sequence'].values[j]\n",
    "                cluster_size = dom_df_ha['cluster_size'].values[j]\n",
    "                total_random_error_ha += min(distance(random_seq_ha_0, domseq), distance(random_seq_ha_1, domseq)) * cluster_size\n",
    "            for j in range(len(dom_df_na)):\n",
    "                domseq = dom_df_na['sequence'].values[j]\n",
    "                cluster_size = dom_df_na['cluster_size'].values[j]\n",
    "                total_random_error_na += min(distance(random_seq_na_0, domseq), distance(random_seq_na_1, domseq)) * cluster_size\n",
    "            random_errors_ha[i].append(total_random_error_ha/sum(dom_df_ha['cluster_size']))\n",
    "            random_errors_na[i].append(total_random_error_na/sum(dom_df_na['cluster_size']))\n",
    "\n",
    "    # mean of the error\n",
    "    df['ha_random_error'] = [np.mean(random_errors_ha[i]) for i in range(21)]\n",
    "    df['na_random_error'] = [np.mean(random_errors_na[i]) for i in range(21)]\n",
    "    # variance of the error\n",
    "    df['ha_random_error_var'] = [np.var(random_errors_ha[i]) for i in range(21)]\n",
    "    df['na_random_error_var'] = [np.var(random_errors_na[i]) for i in range(21)]\n",
    "    # upper bound of the 95% confidence interval of the error\n",
    "    df['ha_random_error_95'] = [np.mean(random_errors_ha[i]) + 1.96*np.std(random_errors_ha[i])/np.sqrt(n) for i in range(21)]\n",
    "    df['na_random_error_95'] = [np.mean(random_errors_na[i]) + 1.96*np.std(random_errors_na[i])/np.sqrt(n) for i in range(21)]\n",
    "    df.to_csv(OUT_DIR + FILE + '_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "505ea1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>north_season</th>\n",
       "      <th>north_h1n1_ha</th>\n",
       "      <th>north_h1n1_na</th>\n",
       "      <th>north_h3n2_ha</th>\n",
       "      <th>north_h3n2_na</th>\n",
       "      <th>south_season</th>\n",
       "      <th>south_h1n1_ha</th>\n",
       "      <th>south_h1n1_na</th>\n",
       "      <th>south_h3n2_ha</th>\n",
       "      <th>south_h3n2_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03_04</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>9.97</td>\n",
       "      <td>3.75</td>\n",
       "      <td>03</td>\n",
       "      <td>4.31</td>\n",
       "      <td>3.94</td>\n",
       "      <td>8.99</td>\n",
       "      <td>-0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04_05</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.03</td>\n",
       "      <td>8.06</td>\n",
       "      <td>04</td>\n",
       "      <td>3.81</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05_06</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>05</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3.81</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06_07</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.86</td>\n",
       "      <td>06</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07_08</td>\n",
       "      <td>5.16</td>\n",
       "      <td>5.64</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-2.68</td>\n",
       "      <td>07</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3.21</td>\n",
       "      <td>4.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08_09</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.63</td>\n",
       "      <td>08</td>\n",
       "      <td>1.89</td>\n",
       "      <td>4.21</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>09_10</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.18</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.35</td>\n",
       "      <td>09</td>\n",
       "      <td>1.50</td>\n",
       "      <td>-24.03</td>\n",
       "      <td>2.39</td>\n",
       "      <td>-1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10_11</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.21</td>\n",
       "      <td>10</td>\n",
       "      <td>36.54</td>\n",
       "      <td>26.85</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11_12</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0.43</td>\n",
       "      <td>11</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>1.71</td>\n",
       "      <td>-1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12_13</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.97</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>12</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>2.53</td>\n",
       "      <td>-3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13_14</td>\n",
       "      <td>4.23</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.48</td>\n",
       "      <td>13</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14_15</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1.40</td>\n",
       "      <td>5.21</td>\n",
       "      <td>1.06</td>\n",
       "      <td>14</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15_16</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>5.84</td>\n",
       "      <td>2.83</td>\n",
       "      <td>15</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.14</td>\n",
       "      <td>6.78</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16_17</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.65</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>5.24</td>\n",
       "      <td>4.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17_18</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.35</td>\n",
       "      <td>17</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18_19</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.17</td>\n",
       "      <td>3.06</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>18</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.02</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19_20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.42</td>\n",
       "      <td>19</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>3.62</td>\n",
       "      <td>-1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20_21</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.39</td>\n",
       "      <td>20</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21_22</td>\n",
       "      <td>4.28</td>\n",
       "      <td>1.41</td>\n",
       "      <td>8.69</td>\n",
       "      <td>3.24</td>\n",
       "      <td>21</td>\n",
       "      <td>3.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>3.46</td>\n",
       "      <td>-6.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22_23</td>\n",
       "      <td>10.26</td>\n",
       "      <td>1.19</td>\n",
       "      <td>3.23</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>22</td>\n",
       "      <td>7.46</td>\n",
       "      <td>1.69</td>\n",
       "      <td>8.08</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23_24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   north_season  north_h1n1_ha  north_h1n1_na  north_h3n2_ha  north_h3n2_na  \\\n",
       "0         03_04           1.45          -0.27           9.97           3.75   \n",
       "1         04_05          -1.88           0.00           4.03           8.06   \n",
       "2         05_06          -1.05          -0.92           1.73          -3.47   \n",
       "3         06_07           0.46           0.76           0.61           0.86   \n",
       "4         07_08           5.16           5.64           0.21          -2.68   \n",
       "5         08_09           2.45           0.47           2.29           1.63   \n",
       "6         09_10           5.66           5.18           1.02           0.35   \n",
       "7         10_11           0.19           0.57           0.84           1.21   \n",
       "8         11_12           1.68          -1.02           5.35           0.43   \n",
       "9         12_13           2.96           1.97           2.65          -0.40   \n",
       "10        13_14           4.23           3.30           3.08           1.48   \n",
       "11        14_15           2.97           1.40           5.21           1.06   \n",
       "12        15_16          -0.27          -0.51           5.84           2.83   \n",
       "13        16_17           2.46           2.26           2.65          -1.33   \n",
       "14        17_18           1.02           0.87           2.34           1.35   \n",
       "15        18_19           1.48           2.17           3.06          -1.23   \n",
       "16        19_20           0.85          -1.11           1.81           1.42   \n",
       "17        20_21           1.24          -0.32           1.24           2.39   \n",
       "18        21_22           4.28           1.41           8.69           3.24   \n",
       "19        22_23          10.26           1.19           3.23          -0.30   \n",
       "20        23_24           0.00           0.00           0.00           0.00   \n",
       "\n",
       "   south_season  south_h1n1_ha  south_h1n1_na  south_h3n2_ha  south_h3n2_na  \n",
       "0            03           4.31           3.94           8.99          -0.37  \n",
       "1            04           3.81           4.35           0.80           2.95  \n",
       "2            05          -0.35           0.44           3.81           5.98  \n",
       "3            06           2.68           1.65           2.71           0.78  \n",
       "4            07           2.17           1.57           3.21           4.42  \n",
       "5            08           1.89           4.21           4.05           4.35  \n",
       "6            09           1.50         -24.03           2.39          -1.51  \n",
       "7            10          36.54          26.85           0.89          -0.68  \n",
       "8            11           0.70          -1.75           1.71          -1.25  \n",
       "9            12           2.08          -2.59           2.53          -3.66  \n",
       "10           13           4.06           0.45           0.79          -4.76  \n",
       "11           14           2.31           1.87           3.32           0.67  \n",
       "12           15           1.12           1.14           6.78           3.17  \n",
       "13           16          -0.52          -1.50           5.24           4.29  \n",
       "14           17           1.45           1.40           1.92           0.10  \n",
       "15           18           3.47           3.40           3.02          -0.02  \n",
       "16           19           0.94          -0.83           3.62          -1.63  \n",
       "17           20           3.52           0.86           5.21           0.70  \n",
       "18           21           3.21          -0.21           3.46          -6.45  \n",
       "19           22           7.46           1.69           8.08          -0.45  \n",
       "20           23           0.00           0.00           0.00           0.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north_errors_df = pd.DataFrame({'north_season':NORTH_YEARS})\n",
    "for FILE in FILES[:2]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    north_errors_df[FILE + '_ha'] = df['ha_random_error_95'] - df['ha_enet_error']\n",
    "    north_errors_df[FILE + '_na'] = df['na_random_error_95'] - df['na_enet_error']\n",
    "    \n",
    "south_errors_df = pd.DataFrame({'south_season':SOUTH_YEARS})\n",
    "for FILE in FILES[2:]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    south_errors_df[FILE + '_ha'] = df['ha_random_error_95'] - df['ha_enet_error']\n",
    "    south_errors_df[FILE + '_na'] = df['na_random_error_95'] - df['ha_enet_error']\n",
    "    \n",
    "errors_df = north_errors_df.join(south_errors_df, how='outer')\n",
    "errors_df.to_csv(OUT_DIR + 'errors_difference_random.csv', index=False)\n",
    "errors_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6e850f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST 2 DECADES\n",
      "north_h1n1\n",
      "    Avg. Improvement HA 2.28\n",
      "    % Improvement HA: 23.37\n",
      "    Avg. Improvement NA 1.15\n",
      "    % Improvement NA: 15.08\n",
      "north_h3n2\n",
      "    Avg. Improvement HA 3.29\n",
      "    % Improvement HA: 59.56\n",
      "    Avg. Improvement NA 1.03\n",
      "    % Improvement NA: 29.03\n",
      "south_h1n1\n",
      "    Avg. Improvement HA 4.12\n",
      "    % Improvement HA: 47.36\n",
      "    Avg. Improvement NA 2.36\n",
      "    % Improvement NA: 31.54\n",
      "south_h3n2\n",
      "    Avg. Improvement HA 3.63\n",
      "    % Improvement HA: 75.14\n",
      "    Avg. Improvement NA 1.6\n",
      "    % Improvement NA: 44.8\n"
     ]
    }
   ],
   "source": [
    "print('LAST 2 DECADES')\n",
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    df['diff_ha'] = df['ha_random_error_95'] - df['ha_enet_error']\n",
    "    df['diff_na'] = df['na_random_error_95'] - df['na_enet_error']\n",
    "    print(FILE)\n",
    "    print('    Avg. Improvement HA', round(sum(df['diff_ha'][:20])/20,2))\n",
    "    print('    % Improvement HA:', round(100*(sum(df['ha_random_error_95'][:20])/sum(df['ha_enet_error'][:20])-1),2))\n",
    "    print('    Avg. Improvement NA', round(sum(df['diff_na'][:20])/20,2))\n",
    "    print('    % Improvement NA:', round(100*(sum(df['na_random_error_95'][:20])/sum(df['na_enet_error'][:20])-1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99a4237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST DECADE\n",
      "north_h1n1\n",
      "    Avg. Improvement HA 2.85\n",
      "    % Improvement HA: 68.89\n",
      "    Avg. Improvement NA 1.07\n",
      "    % Improvement NA: 29.26\n",
      "north_h3n2\n",
      "    Avg. Improvement HA 3.71\n",
      "    % Improvement HA: 58.01\n",
      "    Avg. Improvement NA 1.09\n",
      "    % Improvement NA: 32.05\n",
      "south_h1n1\n",
      "    Avg. Improvement HA 2.7\n",
      "    % Improvement HA: 67.23\n",
      "    Avg. Improvement NA 1.13\n",
      "    % Improvement NA: 30.4\n",
      "south_h3n2\n",
      "    Avg. Improvement HA 4.14\n",
      "    % Improvement HA: 80.27\n",
      "    Avg. Improvement NA 1.25\n",
      "    % Improvement NA: 36.0\n"
     ]
    }
   ],
   "source": [
    "print('LAST DECADE')\n",
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    df['diff_ha'] = df['ha_random_error_95'] - df['ha_enet_error']\n",
    "    df['diff_na'] = df['na_random_error_95'] - df['na_enet_error']\n",
    "    print(FILE)\n",
    "    print('    Avg. Improvement HA', round(sum(df['diff_ha'][10:20])/10,2))\n",
    "    print('    % Improvement HA:', round(100*(sum(df['ha_random_error_95'][10:20])/sum(df['ha_enet_error'][10:20])-1),2))\n",
    "    print('    Avg. Improvement NA', round(sum(df['diff_na'][10:20])/10,2))\n",
    "    print('    % Improvement NA:', round(100*(sum(df['na_random_error_95'][10:20])/sum(df['na_enet_error'][10:20])-1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a052475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
