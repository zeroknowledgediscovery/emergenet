{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "valid-violence",
   "metadata": {},
   "source": [
    "# Influenza Qnet Predictions 2022-2023\n",
    "- Predicting dominant strain for the 2022-2023 flu season using Qnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "refined-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# other\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "\n",
    "# qnet\n",
    "from quasinet.qnet import Qnet, qdistance, qdistance_matrix, membership_degree, save_qnet, load_qnet\n",
    "from quasinet.qseqtools import list_trained_qnets, load_trained_qnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-scheduling",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "- NCBI: https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Protein\n",
    "- GISAID: https://platform.epicov.org/epi3/cfrontend#586f5f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-cargo",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "**GISAID (NCBI has few strains for this season): For creating 2021-2022 Qnets:**\n",
    "1. Download amino acid data from both sources with the following filters:\n",
    "    - Host: Human\n",
    "    - Flu Season: \n",
    "        - Northern strains from 10/01/2021 - 5/01/2022\n",
    "        - Southern strains from 04/01/2021 - 10/1/2021\n",
    "        - Flu season dates from [CDC](https://www.cdc.gov/flu/school-business/travelersfacts.htm)\n",
    "    - Segment: HA (4) and NA (6)\n",
    "2. File names for raw data: HEMISPHERE_SEQUENCE_SEGMENT_SEASON\n",
    "    - HEMISPHERE: \"north\" or \"south\"\n",
    "    - SEQUENCE: \"h1n1\" or \"h3n2\"\n",
    "    - SEGMENT: \"ha\" or \"na\"\n",
    "    - SEASON: year the season begins in (ex. 21 for 2021-2022)\n",
    "    \n",
    "**NCBI (less duplicates): For finding centroid (need all strains):**\n",
    "1. Download amino acid data from both sources with the following filters:\n",
    "    - H1N1 HA: [Link](https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Protein&VirusLineage_ss=H1N1%20subtype,%20taxid:114727&HostLineage_ss=Homo%20sapiens%20(human),%20taxid:9606&ProtNames_ss=hemagglutinin&LabHost_s=include&SLen_i=550%20TO%20600&QualNum_i=0&CollectionDate_dr=2000-01-01T00:00:00.00Z%20TO%202022-05-01T23:59:59.00Z)\n",
    "    - H1N1 NA: [Link](https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Protein&VirusLineage_ss=H1N1%20subtype,%20taxid:114727&HostLineage_ss=Homo%20sapiens%20(human),%20taxid:9606&LabHost_s=include&QualNum_i=0&CollectionDate_dr=2000-01-01T00:00:00.00Z%20TO%202022-05-01T23:59:59.00Z&SLen_i=450%20TO%20500&ProtNames_ss=neuraminidase)\n",
    "    - H3N2 HA: [Link](https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Protein&HostLineage_ss=Homo%20sapiens%20(human),%20taxid:9606&LabHost_s=include&QualNum_i=0&CollectionDate_dr=2000-01-01T00:00:00.00Z%20TO%202022-05-01T23:59:59.00Z&VirusLineage_ss=H3N2%20subtype,%20taxid:119210&SLen_i=550%20TO%20650&ProtNames_ss=hemagglutinin)\n",
    "    - H3N2 NA: [Link](https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Protein&HostLineage_ss=Homo%20sapiens%20(human),%20taxid:9606&LabHost_s=include&QualNum_i=0&CollectionDate_dr=2000-01-01T00:00:00.00Z%20TO%202022-05-01T23:59:59.00Z&SLen_i=450%20TO%20500&ProtNames_ss=neuraminidase&VirusLineage_ss=H3N2%20subtype,%20taxid:119210)\n",
    "    \n",
    "2. File names for raw data: SEQUENCE_SEGMENT\n",
    "    - SEQUENCE: \"h1n1\" or \"h3n2\"\n",
    "    - SEGMENT: \"ha\" or \"na\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "willing-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCBI_PATH = 'raw_data/ncbi/'\n",
    "GISAID_PATH = 'raw_data/gisaid/'\n",
    "\n",
    "FILES = ['north_h1n1_ha_21', 'north_h1n1_na_21', 'north_h3n2_ha_21', 'north_h3n2_na_21',\n",
    "         'south_h1n1_ha_21', 'south_h1n1_na_21', 'south_h3n2_ha_21', 'south_h3n2_na_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "7bf97da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_TRUNC = 469\n",
    "HA_TRUNC = 566"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-letters",
   "metadata": {},
   "source": [
    "## Creating New Qnet\n",
    "- FASTA Header: Isolate name | Type | Segment | Collection date\n",
    "- Truncate NA at 469 amino acids, HA at 566 amino acids\n",
    "- Create Qnet from previous season and hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "recent-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: fasta file name, length to truncate each sequence\n",
    "# output: dataframe of sequences\n",
    "def parse_fasta(file_name, trunc):\n",
    "    acc = []\n",
    "    seq = []\n",
    "    for record in SeqIO.parse(file_name, 'fasta'):\n",
    "        if len(record.seq) < trunc:\n",
    "            continue\n",
    "        acc.append(record.id.split('|')[0])\n",
    "        seq.append(np.array(record.seq[:trunc].upper()))\n",
    "    df = pd.DataFrame({'name':acc, 'sequence':seq})\n",
    "    return df\n",
    "\n",
    "# input: fasta file name, length to truncate each sequence\n",
    "# output: dataframe of sequences, including date as a column\n",
    "def parse_fasta_withdate(file_name, trunc):\n",
    "    acc = []\n",
    "    seq = []\n",
    "    dat = []\n",
    "    for record in SeqIO.parse(file_name, 'fasta'):\n",
    "        if len(record.seq) < trunc:\n",
    "            continue\n",
    "        acc.append(record.id)\n",
    "        dat.append(int(record.description.split('|')[2][:4]))\n",
    "        seq.append(np.array(record.seq[:trunc].upper()))\n",
    "    df = pd.DataFrame({'name':acc, 'sequence':seq, 'year':dat})\n",
    "    return df\n",
    "\n",
    "# input: dataframe of sequences, number of samples\n",
    "# output: array of nucleotide lists\n",
    "def sequence_array(df, sample_size):\n",
    "    seqs = seq_df['sequence'].sample(sample_size, random_state = 42).values\n",
    "    seq_lst = []\n",
    "    for seq in seqs:\n",
    "        seq_lst.append(seq)\n",
    "    return np.array(seq_lst)\n",
    "\n",
    "# input: name to call qnet, array of nucleotide lists, number of nucleotides\n",
    "# output: save qnet as joblib\n",
    "def train_save_qnet(name, seq_arr, num_nuc):\n",
    "    myqnet = Qnet(feature_names=['x'+str(i) for i in np.arange(num_nuc)],n_jobs=1)\n",
    "    myqnet.fit(seq_arr)\n",
    "    save_qnet(myqnet, 'qnet_models/' + name + '.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c37d7fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05eaa6d7c7714a269f83260606364e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create qnets for each dataset\n",
    "for FILE in tqdm(FILES):\n",
    "    TRUNC = HA_TRUNC\n",
    "    if 'na' in FILE:\n",
    "        TRUNC = NA_TRUNC\n",
    "    seq_df = parse_fasta(GISAID_PATH + FILE + \".fasta\", TRUNC) \n",
    "    seq_arr = sequence_array(seq_df, min(1000, len(seq_df)))\n",
    "    train_save_qnet(FILE, seq_arr, TRUNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241b304",
   "metadata": {},
   "source": [
    "## Loading Past Qnet\n",
    "- Show possible Qnets with `list_trained_qnets()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "43d17013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: virus, protein, year\n",
    "# output: qnet \n",
    "def load_influenza_qnet(virus, protein, year):\n",
    "    myqnet = load_trained_qnet('influenza', virus + ';' + protein + ';' + str(year))\n",
    "    TRUNC = HA_TRUNC\n",
    "    if protein == 'na':\n",
    "        TRUNC = NA_TRUNC\n",
    "    # add feature names\n",
    "    myqnet.feature_names=['x'+str(i) for i in np.arange(TRUNC)]\n",
    "    return myqnet\n",
    "\n",
    "# input: list of available years, virus, protein\n",
    "# output: dict of qnets\n",
    "def make_qnet_dict(years, virus, protein):\n",
    "    qnet_dict = {}\n",
    "    for year in years:\n",
    "        qnet_dict[year] = load_influenza_qnet(virus, protein, year)\n",
    "    return qnet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "cfe5d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "H1N1_HA_YEARS = [2000, 2001, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "H1N1_NA_YEARS = [2000, 2001, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "H3N2_HA_YEARS = [2004, 2005, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "H3N2_NA_YEARS = [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "\n",
    "# dict of qnets and years\n",
    "# qnet_dict['h1n1_na']['qnets'][year] accesses a qnet\n",
    "# qnet_dict['h1n1_na']['years'] accesses list of years available\n",
    "qnet_dict = {\n",
    "    'h1n1_ha':{'qnets':make_qnet_dict(H1N1_HA_YEARS, 'h1n1', 'ha'), 'years':H1N1_HA_YEARS},\n",
    "    'h1n1_na':{'qnets':make_qnet_dict(H1N1_NA_YEARS, 'h1n1', 'na'), 'years':H1N1_NA_YEARS},\n",
    "    'h3n2_ha':{'qnets':make_qnet_dict(H3N2_HA_YEARS, 'h3n2', 'ha'), 'years':H3N2_HA_YEARS},\n",
    "    'h3n2_na':{'qnets':make_qnet_dict(H3N2_NA_YEARS, 'h3n2', 'na'), 'years':H3N2_NA_YEARS}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9054f0",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Q-Centroid: $$\\widehat{x}^{t+1} = argmin_{x\\in P} \\sum_{y \\in P^t} \\theta(x,y)$$\n",
    "- Where $P^t$ is the sequence population at time $t$ and $P = P^t \\cup P^{t-1} \\cup P^{t-2} \\cup \\dots \\cup P^1$.\n",
    "- $\\theta(x,y)$ is the qdistance between x and y in their respective Qnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "89412cfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3537dda85d542eea311af3a61492cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rec_files = []\n",
    "rec_names = []\n",
    "rec_seqs = []\n",
    "\n",
    "# set to True to include sequences from P^t-1 U P^t-2 U ... U P^1 - previous populations\n",
    "PAST = False\n",
    "\n",
    "for i in trange(len(FILES)):\n",
    "    FILE = FILES[i]\n",
    "    FILE1 = FILES[(i+4)%8]\n",
    "    NAME = FILE[6:13] # ex. 'h1n1_ha'\n",
    "    \n",
    "    # load this years qnet\n",
    "    myqnet = load_qnet('qnet_models/' + FILE + '.joblib')\n",
    "    \n",
    "    # adjust trunc\n",
    "    TRUNC = HA_TRUNC\n",
    "    if 'na' in FILE:\n",
    "        TRUNC = NA_TRUNC\n",
    "        \n",
    "    # load gisaid data (P^t - population at time t)\n",
    "    gisaid_df1 = parse_fasta(GISAID_PATH + FILE + \".fasta\", TRUNC)\n",
    "    gisaid_df2 = parse_fasta(GISAID_PATH + FILE1 + \".fasta\", TRUNC)\n",
    "    gisaid_df = pd.concat([gisaid_df1, gisaid_df2]).sample(min(1000, len(gisaid_df1) + len(gisaid_df2)), random_state = 42)\n",
    "    # make sequence matrix with gisaid data\n",
    "    cur_seqs_matrix = np.array(list(gisaid_df['sequence'].values))\n",
    "    \n",
    "    cur_rec_names = []\n",
    "    cur_rec_seqs = []\n",
    "    cur_rec_qdist_sums = []\n",
    "    \n",
    "    # loop through available past data\n",
    "    if PAST:\n",
    "        # load ncbi data (P^t-1 U P^t-2 U ... U P^1 - previous populations)\n",
    "        ncbi_df = parse_fasta_withdate(NCBI_PATH + NAME + \".fasta\", TRUNC)\n",
    "        # loops through years with qnet available\n",
    "        for yr in tqdm(qnet_dict[NAME]['years']):\n",
    "            # filter ncbi df by year and drop the year column\n",
    "            df = ncbi_df[ncbi_df['year'] == yr].drop(columns = 'year')\n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            seq_df = df.sample(min(1000, len(df)), random_state = 42)\n",
    "\n",
    "            # compute qdistance matrix\n",
    "            past_seqs_matrix = np.array(list(seq_df['sequence'].values))\n",
    "            dist_matrix = qdistance_matrix(past_seqs_matrix, cur_seqs_matrix, qnet_dict[NAME]['qnets'][yr], myqnet)\n",
    "\n",
    "            # compute q-centroid using formula\n",
    "            sums = list(dist_matrix.sum(axis=1))\n",
    "            cur_min_ind = np.argmin(sums)\n",
    "            cur_rec_name = seq_df.iloc[cur_min_ind].values[0]\n",
    "            cur_rec_seq = seq_df.iloc[cur_min_ind].values[1]\n",
    "\n",
    "            # save to current results\n",
    "            cur_rec_names.append(cur_rec_name)\n",
    "            cur_rec_seqs.append(cur_rec_seq)\n",
    "            cur_rec_qdist_sums.append(min(sums))\n",
    "    \n",
    "    # find centroid of sequences in P^t\n",
    "    dist_matrix = qdistance_matrix(cur_seqs_matrix, cur_seqs_matrix, myqnet, myqnet)\n",
    "    sums = list(dist_matrix.sum(axis=1))\n",
    "    cur_rec_names.append(gisaid_df.iloc[np.argmin(sums)].values[0])\n",
    "    cur_rec_seqs.append(gisaid_df.iloc[np.argmin(sums)].values[1])\n",
    "    cur_rec_qdist_sums.append(min(sums))\n",
    "    \n",
    "    # find centroid among current results\n",
    "    min_ind = np.argmin(cur_rec_qdist_sums)\n",
    "    rec_name = cur_rec_names[min_ind]\n",
    "    rec_seq = cur_rec_seqs[min_ind]\n",
    "    \n",
    "    # save results\n",
    "    rec_files.append(FILE[:13])\n",
    "    rec_names.append(rec_name)\n",
    "    rec_seqs.append(rec_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f96ce184",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rec_seqs)):\n",
    "    rec_seqs[i] = ''.join(rec_seqs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "0ca636ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strain</th>\n",
       "      <th>name</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>north_h1n1_ha_21</td>\n",
       "      <td>A/Seeb/72130049/2021</td>\n",
       "      <td>MKAILVVMLYTFTTANADTLCIGYHANNSTDTVDTVLEKNVTVTHS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>north_h1n1_na_21</td>\n",
       "      <td>A/Netherlands/10642/2022</td>\n",
       "      <td>MNPNQKIITIGSICMAIGTANLILQIGNMISIWVSHSIQIGNQSQI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>north_h3n2_ha_21</td>\n",
       "      <td>A/Arizona/09/2022</td>\n",
       "      <td>MKTIIALSNILCLVFAQKIPGNDNSTATLCLGHHAVPNGTIVKTIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>north_h3n2_na_21</td>\n",
       "      <td>A/Texas/12875/2022</td>\n",
       "      <td>MNPNQKIITIGSVSLTISTICFFMQIAILITTVTLHFKQYEFNSPP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>south_h1n1_ha_21</td>\n",
       "      <td>A/Darwin/3/2022</td>\n",
       "      <td>MKAILVVMLYTFTTANADTLCIGYHANNSTDTVDTVLEKNVTVTHS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>south_h1n1_na_21</td>\n",
       "      <td>A/Netherlands/00124/2021</td>\n",
       "      <td>MNPNQKIITIGSICMAIGTANLILQIGNIISIWVSHSIQTGNQSQI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>south_h3n2_ha_21</td>\n",
       "      <td>A/Niger/10180/2021</td>\n",
       "      <td>MKTIIALSNILCLVFAQKIPGNDNSTATLCLGHHAVPNGTIVKTIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>south_h3n2_na_21</td>\n",
       "      <td>A/Texas/12723/2022</td>\n",
       "      <td>MNPNQKIITIGSVSLTISTICFFMQIAILITTVTLHFKQYEFNSPP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             strain                      name  \\\n",
       "0  north_h1n1_ha_21      A/Seeb/72130049/2021   \n",
       "1  north_h1n1_na_21  A/Netherlands/10642/2022   \n",
       "2  north_h3n2_ha_21         A/Arizona/09/2022   \n",
       "3  north_h3n2_na_21        A/Texas/12875/2022   \n",
       "4  south_h1n1_ha_21           A/Darwin/3/2022   \n",
       "5  south_h1n1_na_21  A/Netherlands/00124/2021   \n",
       "6  south_h3n2_ha_21        A/Niger/10180/2021   \n",
       "7  south_h3n2_na_21        A/Texas/12723/2022   \n",
       "\n",
       "                                            sequence  \n",
       "0  MKAILVVMLYTFTTANADTLCIGYHANNSTDTVDTVLEKNVTVTHS...  \n",
       "1  MNPNQKIITIGSICMAIGTANLILQIGNMISIWVSHSIQIGNQSQI...  \n",
       "2  MKTIIALSNILCLVFAQKIPGNDNSTATLCLGHHAVPNGTIVKTIT...  \n",
       "3  MNPNQKIITIGSVSLTISTICFFMQIAILITTVTLHFKQYEFNSPP...  \n",
       "4  MKAILVVMLYTFTTANADTLCIGYHANNSTDTVDTVLEKNVTVTHS...  \n",
       "5  MNPNQKIITIGSICMAIGTANLILQIGNIISIWVSHSIQTGNQSQI...  \n",
       "6  MKTIIALSNILCLVFAQKIPGNDNSTATLCLGHHAVPNGTIVKTIT...  \n",
       "7  MNPNQKIITIGSVSLTISTICFFMQIAILITTVTLHFKQYEFNSPP...  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame({'strain':rec_files, 'name':rec_names, 'sequence':rec_seqs})\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "2d8d90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe as csv\n",
    "os.makedirs('results', exist_ok=True)  \n",
    "predictions.to_csv('results/influenza_qnet_predictions_2022_2023.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cad32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
