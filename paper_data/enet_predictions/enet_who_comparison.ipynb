{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eada972",
   "metadata": {},
   "source": [
    "# Enet Predictions vs. WHO Predictions\n",
    "- Compare Enet predictions and WHO predictions\n",
    "- For each season, take the average levenshtein distance between the prediction and the top 10 dominant strains\n",
    "- Truncate sequence to HA 565, NA 468\n",
    "- WHO predictions from [WHO vaccine recommendations](https://www.who.int/teams/global-influenza-programme/vaccines/who-recommendations/recommendations-for-influenza-vaccine-composition-archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ab0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from Levenshtein import distance\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5a547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHO_DIR = 'raw_data/who/'\n",
    "PRED_DIR = 'results/enet_predictions/'\n",
    "DOM_DIR = 'results/dominant_sequences/'\n",
    "OUT_DIR = 'results/enet_who_comparison/'\n",
    "\n",
    "FILES = ['north_h1n1', 'north_h3n2', 'south_h1n1', 'south_h3n2']\n",
    "\n",
    "NORTH_YEARS = []\n",
    "for i in np.arange(2, 23):\n",
    "    YEAR = ''\n",
    "    if i < 10:\n",
    "        YEAR += '0' + str(i)\n",
    "    else:\n",
    "        YEAR += (str(i))\n",
    "    if i + 1 < 10:\n",
    "        YEAR += '_0' + str(i + 1)\n",
    "    else:\n",
    "        YEAR += '_' + str(i + 1)\n",
    "    NORTH_YEARS.append(YEAR)\n",
    "        \n",
    "SOUTH_YEARS = []\n",
    "for i in np.arange(3, 24):\n",
    "    if i < 10:\n",
    "        SOUTH_YEARS.append('0' + str(i))\n",
    "    else:\n",
    "        SOUTH_YEARS.append(str(i))\n",
    "\n",
    "NA_TRUNC = 468 # 2 less than official length of 470\n",
    "HA_TRUNC = 565 # 2 less than official length of 567"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f15349",
   "metadata": {},
   "source": [
    "## Construct DataFrame\n",
    "- `season`: 02-03 through 22-23 for north, 03 through 23 for south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e959c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:    \n",
    "    df = pd.read_csv(PRED_DIR + FILE + '_predictions.csv')\n",
    "    \n",
    "    # WHO recommendation name, sequence\n",
    "    who_ha_df = pd.read_csv(WHO_DIR + FILE + '_ha.csv')\n",
    "    who_na_df = pd.read_csv(WHO_DIR + FILE + '_na.csv')\n",
    "    df['name_who'] = who_ha_df['who_recommendation_name']\n",
    "    df['ha_seq_who'] = who_ha_df['who_recommendation_sequence']\n",
    "    df['na_seq_who'] = who_na_df['who_recommendation_sequence']\n",
    "    \n",
    "    # error columns\n",
    "    for i in range(3):\n",
    "        df['ha_enet_error_' + str(i)] = -1 * np.ones(len(df))\n",
    "        df['na_enet_error_' + str(i)] = -1 * np.ones(len(df))\n",
    "    df['ha_who_error'] = -1 * np.ones(len(df))\n",
    "    df['na_who_error'] = -1 * np.ones(len(df))\n",
    "    \n",
    "    df.to_csv(OUT_DIR + FILE + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d1fde",
   "metadata": {},
   "source": [
    "## Compute Enet and WHO Errors\n",
    "- For each season, take the average Levenshtein distance between the prediction and the top 10 dominant strains\n",
    "    - Dominant strains in `results/dominant_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f59d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    seasons = df['season'].values\n",
    "    \n",
    "    for i in range(3):\n",
    "        enet_errors_ha = []\n",
    "        enet_errors_na = []\n",
    "        who_errors_ha = []\n",
    "        who_errors_na = []\n",
    "        for season in seasons:\n",
    "            season_str = str(season)\n",
    "            if len(season_str) == 1:\n",
    "                season_str = str('0' + season_str)\n",
    "            # read dominant sequences\n",
    "            DATA_DIR_HA = DOM_DIR + FILE + '_ha/' + FILE + '_ha_' + season_str + '.csv'\n",
    "            DATA_DIR_NA = DOM_DIR + FILE + '_na/' + FILE + '_na_' + season_str + '.csv'\n",
    "            if not os.path.isfile(DATA_DIR_HA):\n",
    "                enet_errors_ha.append(-1)\n",
    "                enet_errors_na.append(-1)\n",
    "                who_errors_ha.append(-1)\n",
    "                who_errors_na.append(-1)\n",
    "                continue\n",
    "            dom_df_ha = pd.read_csv(DATA_DIR_HA, index_col=0)\n",
    "            dom_df_na = pd.read_csv(DATA_DIR_NA, index_col=0)\n",
    "            top_dominant_seqs_ha = dom_df_ha['sequence'].values[:10]\n",
    "            top_dominant_seqs_na = dom_df_na['sequence'].values[:10]\n",
    "            # access enet and who recommendations\n",
    "            enet_ha_seq = df[df['season'] == season]['ha_seq_' + str(i)].values[0][:HA_TRUNC]\n",
    "            enet_na_seq = df[df['season'] == season]['na_seq_' + str(i)].values[0][:NA_TRUNC]\n",
    "            who_ha_seq = df[df['season'] == season]['ha_seq_who'].values[0][:HA_TRUNC]\n",
    "            who_na_seq = df[df['season'] == season]['na_seq_who'].values[0][:NA_TRUNC]\n",
    "            # find average enet and who errors\n",
    "            total_enet_error_ha = 0\n",
    "            total_enet_error_na = 0\n",
    "            total_who_error_ha = 0\n",
    "            total_who_error_na = 0\n",
    "            for domseq in top_dominant_seqs_ha:\n",
    "                total_enet_error_ha += distance(enet_ha_seq, domseq[:HA_TRUNC])\n",
    "                total_who_error_ha += distance(who_ha_seq, domseq[:HA_TRUNC])\n",
    "            for domseq in top_dominant_seqs_na:\n",
    "                total_enet_error_na += distance(enet_na_seq, domseq[:NA_TRUNC])\n",
    "                total_who_error_na += distance(who_na_seq, domseq[:NA_TRUNC])\n",
    "            enet_errors_ha.append(total_enet_error_ha/10)\n",
    "            enet_errors_na.append(total_enet_error_na/10)\n",
    "            who_errors_ha.append(total_who_error_ha/10)\n",
    "            who_errors_na.append(total_who_error_na/10)\n",
    "\n",
    "        # add to dataframe\n",
    "        df['ha_enet_error_' + str(i)] = enet_errors_ha\n",
    "        df['na_enet_error_' + str(i)] = enet_errors_na\n",
    "        df['ha_who_error'] = who_errors_ha\n",
    "        df['na_who_error'] = who_errors_na\n",
    "    \n",
    "    df['best_ha_enet_error'] = df[['ha_enet_error_0','ha_enet_error_1']].min(axis=1)\n",
    "    df['best_na_enet_error'] = df[['na_enet_error_0','na_enet_error_1']].min(axis=1)\n",
    "    df.to_csv(OUT_DIR + FILE + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b638ae69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>north_season</th>\n",
       "      <th>north_h1n1_ha</th>\n",
       "      <th>north_h1n1_na</th>\n",
       "      <th>north_h3n2_ha</th>\n",
       "      <th>north_h3n2_na</th>\n",
       "      <th>south_season</th>\n",
       "      <th>south_h1n1_ha</th>\n",
       "      <th>south_h1n1_na</th>\n",
       "      <th>south_h3n2_ha</th>\n",
       "      <th>south_h3n2_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02_03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>03</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03_04</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>04</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04_05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>05</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05_06</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>06</td>\n",
       "      <td>7.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06_07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>07</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07_08</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>08</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>08_09</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>09_10</td>\n",
       "      <td>118.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10_11</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11_12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12_13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13_14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14_15</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15_16</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16_17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17_18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18_19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19_20</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20_21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21_22</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22_23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   north_season  north_h1n1_ha  north_h1n1_na  north_h3n2_ha  north_h3n2_na  \\\n",
       "0         02_03            3.0            1.0           17.0           13.0   \n",
       "1         03_04            4.0           -1.0           20.0           10.0   \n",
       "2         04_05            5.0            1.0            3.0            2.0   \n",
       "3         05_06            4.6            3.0            5.0            0.0   \n",
       "4         06_07            4.0            3.0           -1.0           -2.0   \n",
       "5         07_08           -4.0            2.0            3.0            7.0   \n",
       "6         08_09           -2.0            0.0            3.0            1.0   \n",
       "7         09_10          118.0           89.0            2.0            2.0   \n",
       "8         10_11            4.0            2.0            5.0            2.0   \n",
       "9         11_12            7.0            4.0            1.0            1.0   \n",
       "10        12_13            5.0            1.0           -1.0            2.0   \n",
       "11        13_14           10.0            3.0            6.0            2.0   \n",
       "12        14_15           10.0            7.0            6.0            1.0   \n",
       "13        15_16           11.0            8.0           10.0            1.0   \n",
       "14        16_17           15.0           14.0           -1.0            4.0   \n",
       "15        17_18            2.0           -1.0            2.0            7.0   \n",
       "16        18_19            5.0            3.0           -6.0           -1.1   \n",
       "17        19_20            7.0            6.0            3.0            7.0   \n",
       "18        20_21            5.0            0.0           -1.6           -2.0   \n",
       "19        21_22           -2.0          -13.0            0.0            0.0   \n",
       "20        22_23            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   south_season  south_h1n1_ha  south_h1n1_na  south_h3n2_ha  south_h3n2_na  \n",
       "0            03            3.0            1.0           23.0            9.0  \n",
       "1            04            4.3            1.0            3.0            1.0  \n",
       "2            05            5.4            2.0            0.0            2.0  \n",
       "3            06            7.4            5.0            5.8            0.5  \n",
       "4            07            8.0            2.0            4.0            6.0  \n",
       "5            08            7.0           24.0            1.0           -1.0  \n",
       "6            09            2.0            0.0            2.0            0.0  \n",
       "7            10            1.0            2.0            3.0            0.0  \n",
       "8            11            3.0            1.0           -2.0            3.0  \n",
       "9            12            1.8            1.0            4.0            5.0  \n",
       "10           13            4.0            0.0            3.0            2.0  \n",
       "11           14            9.0            7.0            3.0           -1.0  \n",
       "12           15            8.0            6.0           10.0            3.0  \n",
       "13           16            9.0            9.0            3.0            8.0  \n",
       "14           17            2.0           -1.0            0.0            6.6  \n",
       "15           18            4.0            3.0            2.0            2.0  \n",
       "16           19            4.0            4.0            2.0            3.0  \n",
       "17           20            6.3            6.0           -2.0           -3.0  \n",
       "18           21            5.0           -5.0           18.0            3.0  \n",
       "19           22            4.0            0.0            3.0            2.0  \n",
       "20           23            0.0            0.0            0.0            0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north_errors_df = pd.DataFrame({'north_season':NORTH_YEARS})\n",
    "for FILE in FILES[:2]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    errors_ha = []\n",
    "    errors_na = []\n",
    "    # take minimum error of two largest clusters\n",
    "    for i in range(len(df)):\n",
    "        errors_ha.append(df['ha_who_error'][i] - df['best_ha_enet_error'][i])\n",
    "        errors_na.append(df['na_who_error'][i] - df['best_na_enet_error'][i])\n",
    "    north_errors_df[FILE + '_ha'] = errors_ha\n",
    "    north_errors_df[FILE + '_na'] = errors_na\n",
    "    \n",
    "south_errors_df = pd.DataFrame({'south_season':SOUTH_YEARS})\n",
    "for FILE in FILES[2:]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    errors_ha = []\n",
    "    errors_na = []\n",
    "    # take minimum error of two largest clusters\n",
    "    for i in range(len(df)):\n",
    "        errors_ha.append(df['ha_who_error'][i] - df['best_ha_enet_error'][i])\n",
    "        errors_na.append(df['na_who_error'][i] - df['best_na_enet_error'][i])\n",
    "    south_errors_df[FILE + '_ha'] = errors_ha\n",
    "    south_errors_df[FILE + '_na'] = errors_na\n",
    "    \n",
    "errors_df = north_errors_df.join(south_errors_df, how='outer')\n",
    "errors_df.to_csv(OUT_DIR + 'errors_difference.csv', index=False)\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96aba3",
   "metadata": {},
   "source": [
    "## What if we used a random strain from that season instead of our predicted strain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dc5d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:    \n",
    "    df = pd.read_csv(PRED_DIR + FILE + '_predictions.csv')[['season']]\n",
    "    \n",
    "    # WHO recommendation name, sequence\n",
    "    who_ha_df = pd.read_csv(WHO_DIR + FILE + '_ha.csv')\n",
    "    #who_na_df = pd.read_csv(WHO_DIR + FILE + '_na.csv')\n",
    "    df['name_who'] = who_ha_df['who_recommendation_name']\n",
    "    df['ha_seq_who'] = who_ha_df['who_recommendation_sequence']\n",
    "    #df['na_seq_who'] = who_na_df['who_recommendation_sequence']\n",
    "    \n",
    "    # error columns\n",
    "    df['ha_random_error'] = -1 * np.ones(len(df))\n",
    "    #df['na_enet_error'] = -1 * np.ones(len(df))\n",
    "    df['ha_who_error'] = -1 * np.ones(len(df))\n",
    "    #df['na_who_error'] = -1 * np.ones(len(df))\n",
    "    \n",
    "    df.to_csv(OUT_DIR + FILE + '_random.csv', index=False)\n",
    "    \n",
    "NORTH_YEARS_RANDOM = []\n",
    "for i in np.arange(1, 22):\n",
    "    YEAR = ''\n",
    "    if i < 10:\n",
    "        YEAR += '0' + str(i)\n",
    "    else:\n",
    "        YEAR += (str(i))\n",
    "    if i + 1 < 10:\n",
    "        YEAR += '_0' + str(i + 1)\n",
    "    else:\n",
    "        YEAR += '_' + str(i + 1)\n",
    "    NORTH_YEARS_RANDOM.append(YEAR)\n",
    "        \n",
    "SOUTH_YEARS_RANDOM = []\n",
    "for i in np.arange(2, 23):\n",
    "    if i < 10:\n",
    "        SOUTH_YEARS_RANDOM.append('0' + str(i))\n",
    "    else:\n",
    "        SOUTH_YEARS_RANDOM.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "63daa0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for FILE in FILES:\n",
    "    YEARS = NORTH_YEARS_RANDOM\n",
    "    if FILE[:5] == 'south':\n",
    "        YEARS = SOUTH_YEARS_RANDOM\n",
    "    random_seqs = []\n",
    "    for YEAR in YEARS:\n",
    "        population_df = pd.read_csv('raw_data/merged/' + FILE + '/' + FILE + '_ha_' + YEAR + '.csv')\n",
    "        random_seq = population_df.sample(1, random_state=0)['sequence'].values[0]\n",
    "        random_seqs.append(random_seq)\n",
    "    \n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    seasons = df['season'].values\n",
    "    \n",
    "    random_errors_ha = []\n",
    "    #random_errors_na = []\n",
    "    who_errors_ha = []\n",
    "    #who_errors_na = []\n",
    "        \n",
    "    for i in range(len(seasons)):\n",
    "        season_str = str(seasons[i])\n",
    "        if len(season_str) == 1:\n",
    "            season_str = str('0' + season_str)\n",
    "        # read dominant sequences\n",
    "        DATA_DIR_HA = DOM_DIR + FILE + '_ha/' + FILE + '_ha_' + season_str + '.csv'\n",
    "        #DATA_DIR_NA = DOM_DIR + FILE + '_na/' + FILE + '_na_' + season_str + '.csv'\n",
    "        if not os.path.isfile(DATA_DIR_HA):\n",
    "            random_errors_ha.append(-1)\n",
    "            #random_errors_na.append(-1)\n",
    "            who_errors_ha.append(-1)\n",
    "            #who_errors_na.append(-1)\n",
    "            continue\n",
    "        dom_df_ha = pd.read_csv(DATA_DIR_HA)\n",
    "        #dom_df_na = pd.read_csv(DATA_DIR_NA)\n",
    "        top_dominant_seqs_ha = dom_df_ha['sequence'].values[:10]\n",
    "        #top_dominant_seqs_na = dom_df_na['sequence'].values[:10]\n",
    "    \n",
    "        # access who recommendations\n",
    "        who_ha_seq = df[df['season'] == seasons[i]]['ha_seq_who'].values[0][:HA_TRUNC]\n",
    "        #who_na_seq = df[df['season'] == seasons[i]]['na_seq_who'].values[0][:NA_TRUNC]\n",
    "\n",
    "        # random seq\n",
    "        random_seq_ha = random_seqs[i][:HA_TRUNC]\n",
    "            \n",
    "        # find average enet and who errors\n",
    "        total_random_error_ha = 0\n",
    "        #total_random_error_na = 0\n",
    "        total_who_error_ha = 0\n",
    "        #total_who_error_na = 0\n",
    "        for domseq in top_dominant_seqs_ha:\n",
    "            total_random_error_ha += distance(random_seq_ha, domseq[:HA_TRUNC])\n",
    "            total_who_error_ha += distance(who_ha_seq, domseq[:HA_TRUNC])\n",
    "#         for domseq in top_dominant_seqs_na:\n",
    "#             total_random_error_na += distance(random_seq_na, domseq[:NA_TRUNC])\n",
    "#             total_who_error_na += distance(who_na_seq, domseq[:NA_TRUNC])\n",
    "        random_errors_ha.append(total_random_error_ha/10)\n",
    "        # random_errors_na.append(total_random_error_na/10)\n",
    "        who_errors_ha.append(total_who_error_ha/10)\n",
    "        # who_errors_na.append(total_who_error_na/10)\n",
    "\n",
    "    # add to dataframe\n",
    "    df['ha_random_error'] = random_errors_ha\n",
    "    # df['na_random_error'] = random_errors_na\n",
    "    df['ha_who_error'] = who_errors_ha\n",
    "    # df['na_who_error'] = who_errors_na\n",
    "    \n",
    "    df.to_csv(OUT_DIR + FILE + '_random.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e339b967",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>north_season</th>\n",
       "      <th>north_h1n1_ha</th>\n",
       "      <th>north_h3n2_ha</th>\n",
       "      <th>south_season</th>\n",
       "      <th>south_h1n1_ha</th>\n",
       "      <th>south_h3n2_ha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02_03</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>03</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03_04</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04_05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05_06</td>\n",
       "      <td>3.8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>06</td>\n",
       "      <td>1.4</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06_07</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07_08</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>08</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>08_09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>09_10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10_11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11_12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12_13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13_14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14_15</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15_16</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16_17</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17_18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18_19</td>\n",
       "      <td>4.6</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19_20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>7.1</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20_21</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21_22</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22_23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   north_season  north_h1n1_ha  north_h3n2_ha south_season  south_h1n1_ha  \\\n",
       "0         02_03           -4.0            7.0           03           -4.0   \n",
       "1         03_04            4.0           23.0           04            0.8   \n",
       "2         04_05            1.0            0.0           05           -3.0   \n",
       "3         05_06            3.8           11.0           06            1.4   \n",
       "4         06_07           -7.0            3.0           07            2.0   \n",
       "5         07_08           -5.0            6.0           08            7.0   \n",
       "6         08_09           -1.0            3.0           09           -1.0   \n",
       "7         09_10            0.0            1.0           10            2.0   \n",
       "8         10_11            1.0           -1.0           11            3.0   \n",
       "9         11_12            2.0            0.0           12            3.0   \n",
       "10        12_13            1.0          -10.0           13            3.0   \n",
       "11        13_14            4.0            3.0           14            9.0   \n",
       "12        14_15           10.0           -2.0           15            3.0   \n",
       "13        15_16           11.0           10.0           16           11.0   \n",
       "14        16_17           16.0          -12.0           17            0.0   \n",
       "15        17_18            2.0           -7.0           18            5.0   \n",
       "16        18_19            4.6           -6.0           19            5.0   \n",
       "17        19_20            6.0            0.0           20            7.1   \n",
       "18        20_21            7.0          -12.0           21            5.0   \n",
       "19        21_22            6.0            7.0           22          -10.0   \n",
       "20        22_23            0.0            0.0           23            0.0   \n",
       "\n",
       "    south_h3n2_ha  \n",
       "0             5.0  \n",
       "1             1.0  \n",
       "2             2.0  \n",
       "3             9.8  \n",
       "4             5.0  \n",
       "5             0.0  \n",
       "6             0.0  \n",
       "7             1.0  \n",
       "8            -3.0  \n",
       "9             9.0  \n",
       "10          -14.0  \n",
       "11            2.0  \n",
       "12           10.0  \n",
       "13           -2.0  \n",
       "14            1.4  \n",
       "15            0.0  \n",
       "16           -3.0  \n",
       "17           -2.0  \n",
       "18           18.0  \n",
       "19            3.0  \n",
       "20            0.0  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "north_errors_df = pd.DataFrame({'north_season':NORTH_YEARS})\n",
    "for FILE in FILES[:2]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    errors_ha = []\n",
    "    # errors_na = []\n",
    "    # take minimum error of two largest clusters\n",
    "    for i in range(len(df)):\n",
    "        errors_ha.append(df['ha_who_error'][i] - df['ha_random_error'][i])\n",
    "        # errors_na.append(df['na_who_error'][i] - df['best_na_enet_error'][i])\n",
    "    north_errors_df[FILE + '_ha'] = errors_ha\n",
    "    # north_errors_df[FILE + '_na'] = errors_na\n",
    "    \n",
    "south_errors_df = pd.DataFrame({'south_season':SOUTH_YEARS})\n",
    "for FILE in FILES[2:]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_random.csv')\n",
    "    errors_ha = []\n",
    "    # errors_na = []\n",
    "    # take minimum error of two largest clusters\n",
    "    for i in range(len(df)):\n",
    "        errors_ha.append(df['ha_who_error'][i] - df['ha_random_error'][i])\n",
    "        # errors_na.append(df['na_who_error'][i] - df['best_na_enet_error'][i])\n",
    "    south_errors_df[FILE + '_ha'] = errors_ha\n",
    "    # south_errors_df[FILE + '_na'] = errors_na\n",
    "    \n",
    "errors_df = north_errors_df.join(south_errors_df, how='outer')\n",
    "errors_df.to_csv(OUT_DIR + 'errors_difference_random.csv', index=False)\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2e5884c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north_h1n1\n",
      "    Avg. Improvement 7.46\n",
      "    % Improvement: 179.98\n",
      "north_h3n2\n",
      "    Avg. Improvement 2.57\n",
      "    % Improvement: 40.99\n",
      "south_h1n1\n",
      "    Avg. Improvement 2.45\n",
      "    % Improvement: 24.17\n",
      "south_h3n2\n",
      "    Avg. Improvement 2.13\n",
      "    % Improvement: 45.46\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    df1 = pd.read_csv(OUT_DIR + FILES[i] + '_random.csv')[['season','ha_random_error']]\n",
    "    df2 = pd.read_csv(OUT_DIR + FILES[i] + '.csv')[['season','ha_who_error','best_ha_enet_error']]\n",
    "    df1 = df1.merge(df2, on='season')\n",
    "    df1['diff'] = df1['ha_random_error'] - df1['best_ha_enet_error']\n",
    "    print(FILES[i])\n",
    "    print('    Avg. Improvement', round(sum(df1['diff'][:20])/20, 2))\n",
    "    print('    % Improvement:', round(100*(sum(df1['ha_random_error'][:20])/sum(df1['best_ha_enet_error'][:20])-1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f5c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
