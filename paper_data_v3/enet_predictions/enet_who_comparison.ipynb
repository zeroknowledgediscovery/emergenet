{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eada972",
   "metadata": {},
   "source": [
    "# Enet Predictions vs. WHO Predictions\n",
    "Compare Enet predictions and WHO predictions.\n",
    "- For each season, take the average Hamming distance between the predictions and all strains\n",
    "- Truncate sequence to HA 565, NA 468\n",
    "- WHO predictions from [WHO vaccine recommendations](https://www.who.int/teams/global-influenza-programme/vaccines/who-recommendations/recommendations-for-influenza-vaccine-composition-archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5a547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from distance import hamming as distance\n",
    "pd.set_option('display.max_columns', None)\n",
    "from domseq import DomSeq\n",
    "from zedstat.textable import textable\n",
    "\n",
    "\n",
    "WHO_DIR = 'data/who/'\n",
    "SEASONAL_DIR = 'results/enet_predictions/seasonal_predictions/'\n",
    "PRED_DIR = 'results/enet_predictions/'\n",
    "OUT_DIR = 'results/enet_who_comparison/'\n",
    "TAB_DIR = 'tables/plotdata/'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "FILES = ['north_h1n1', 'north_h3n2', 'south_h1n1', 'south_h3n2']\n",
    "\n",
    "NORTH_YEARS = []\n",
    "for i in np.arange(3, 24):\n",
    "    YEAR = ''\n",
    "    if i < 10:\n",
    "        YEAR += '0' + str(i)\n",
    "    else:\n",
    "        YEAR += (str(i))\n",
    "    if i + 1 < 10:\n",
    "        YEAR += '_0' + str(i + 1)\n",
    "    else:\n",
    "        YEAR += '_' + str(i + 1)\n",
    "    NORTH_YEARS.append(YEAR)\n",
    "        \n",
    "SOUTH_YEARS = []\n",
    "for i in np.arange(3, 24):\n",
    "    if i < 10:\n",
    "        SOUTH_YEARS.append('0' + str(i))\n",
    "    else:\n",
    "        SOUTH_YEARS.append(str(i))\n",
    "\n",
    "NA_TRUNC = 468 # 2 less than official length of 470\n",
    "HA_TRUNC = 565 # 2 less than official length of 567"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d1fde",
   "metadata": {},
   "source": [
    "## Compute Enet and WHO Errors\n",
    "For each season, take the average Hamming distance between the prediction and all strains\n",
    "- `season`: 03-04 through 23-24 for north, 03 through 23 for south\n",
    "- Do this for both multicluster and single cluster predictions\n",
    "    - Two-cluster: `results/enet_predictions/<hemisphere>_<subtype>_predictions.csv`\n",
    "    - Single-cluster: `results/enet_predictions/<hemisphere>_<subtype>_predictions_single_cluster.csv`\n",
    "\n",
    "\n",
    "## Two-Cluster Predictions vs. WHO\n",
    "- For each sequence in a season, take the minimum Hamming distance between that sequence and our predictions from the two largest clusters\n",
    "- **Note: in the 2009-10 H1N1 Northern flu season, we outperform WHO by 423.79 edits**\n",
    "    - Our predictions are A/Hawaii/02/2008,2008-01-17 (ACF10336.1, cluster size 563) and A/Hong Kong/H090-751-V3 (EPI326467, cluster size 23)\n",
    "    - The latter is 423.79 edits closer to the population than WHO\n",
    "    - The sequence was collected on 2009-02-08, so technically before the WHO recommendation date, but was only submitted on 2011-07-08, so we exclude it from our report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870e37ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Improvement\n",
      "\tNorth H1N1 HA: 24.430\n",
      "\tNorth H3N2 HA: 4.414\n",
      "\tSouth H1N1 HA: 5.038\n",
      "\tSouth H3N2 HA: 3.272\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>north_season</th>\n",
       "      <th>north_h1n1_ha</th>\n",
       "      <th>north_h3n2_ha</th>\n",
       "      <th>south_season</th>\n",
       "      <th>south_h1n1_ha</th>\n",
       "      <th>south_h3n2_ha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03_04</td>\n",
       "      <td>3.00</td>\n",
       "      <td>22.72</td>\n",
       "      <td>03</td>\n",
       "      <td>4.31</td>\n",
       "      <td>19.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04_05</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>4.75</td>\n",
       "      <td>04</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05_06</td>\n",
       "      <td>4.18</td>\n",
       "      <td>7.90</td>\n",
       "      <td>05</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06_07</td>\n",
       "      <td>4.75</td>\n",
       "      <td>5.22</td>\n",
       "      <td>06</td>\n",
       "      <td>3.30</td>\n",
       "      <td>10.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07_08</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>6.85</td>\n",
       "      <td>07</td>\n",
       "      <td>6.47</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08_09</td>\n",
       "      <td>1.14</td>\n",
       "      <td>2.62</td>\n",
       "      <td>08</td>\n",
       "      <td>6.42</td>\n",
       "      <td>-0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>09_10</td>\n",
       "      <td>423.79</td>\n",
       "      <td>1.97</td>\n",
       "      <td>09</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10_11</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.41</td>\n",
       "      <td>10</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11_12</td>\n",
       "      <td>5.18</td>\n",
       "      <td>2.85</td>\n",
       "      <td>11</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12_13</td>\n",
       "      <td>4.31</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>12</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13_14</td>\n",
       "      <td>8.69</td>\n",
       "      <td>4.80</td>\n",
       "      <td>13</td>\n",
       "      <td>6.81</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14_15</td>\n",
       "      <td>8.85</td>\n",
       "      <td>7.78</td>\n",
       "      <td>14</td>\n",
       "      <td>9.52</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15_16</td>\n",
       "      <td>9.87</td>\n",
       "      <td>4.92</td>\n",
       "      <td>15</td>\n",
       "      <td>10.80</td>\n",
       "      <td>-0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16_17</td>\n",
       "      <td>14.19</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>16</td>\n",
       "      <td>12.61</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17_18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>17</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18_19</td>\n",
       "      <td>4.06</td>\n",
       "      <td>2.20</td>\n",
       "      <td>18</td>\n",
       "      <td>4.94</td>\n",
       "      <td>3.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19_20</td>\n",
       "      <td>5.52</td>\n",
       "      <td>8.54</td>\n",
       "      <td>19</td>\n",
       "      <td>4.95</td>\n",
       "      <td>5.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20_21</td>\n",
       "      <td>6.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>20</td>\n",
       "      <td>5.68</td>\n",
       "      <td>-2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21_22</td>\n",
       "      <td>2.40</td>\n",
       "      <td>8.86</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>9.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22_23</td>\n",
       "      <td>5.46</td>\n",
       "      <td>1.05</td>\n",
       "      <td>22</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23_24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   north_season  north_h1n1_ha  north_h3n2_ha south_season  south_h1n1_ha  \\\n",
       "0         03_04           3.00          22.72           03           4.31   \n",
       "1         04_05          -2.29           4.75           04           2.50   \n",
       "2         05_06           4.18           7.90           05           2.94   \n",
       "3         06_07           4.75           5.22           06           3.30   \n",
       "4         07_08          -0.62           6.85           07           6.47   \n",
       "5         08_09           1.14           2.62           08           6.42   \n",
       "6         09_10         423.79           1.97           09           0.87   \n",
       "7         10_11           3.09           2.41           10           7.35   \n",
       "8         11_12           5.18           2.85           11           3.75   \n",
       "9         12_13           4.31          -1.01           12           5.74   \n",
       "10        13_14           8.69           4.80           13           6.81   \n",
       "11        14_15           8.85           7.78           14           9.52   \n",
       "12        15_16           9.87           4.92           15          10.80   \n",
       "13        16_17          14.19          -0.64           16          12.61   \n",
       "14        17_18           1.00          -1.33           17           0.98   \n",
       "15        18_19           4.06           2.20           18           4.94   \n",
       "16        19_20           5.52           8.54           19           4.95   \n",
       "17        20_21           6.46           0.25           20           5.68   \n",
       "18        21_22           2.40           8.86           21          -0.40   \n",
       "19        22_23           5.46           1.05           22           6.25   \n",
       "20        23_24           0.00           0.00           23           0.00   \n",
       "\n",
       "    south_h3n2_ha  \n",
       "0           19.36  \n",
       "1            2.03  \n",
       "2            2.63  \n",
       "3           10.70  \n",
       "4            3.74  \n",
       "5           -0.88  \n",
       "6            2.00  \n",
       "7            0.03  \n",
       "8            4.52  \n",
       "9            5.16  \n",
       "10          -0.77  \n",
       "11           2.33  \n",
       "12          -0.73  \n",
       "13           1.59  \n",
       "14          -0.62  \n",
       "15           3.19  \n",
       "16           5.71  \n",
       "17          -2.26  \n",
       "18           9.20  \n",
       "19           1.80  \n",
       "20           0.00  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for FILE in FILES:    \n",
    "    # Prepare prediction DataFrame\n",
    "    df = pd.read_csv(PRED_DIR + FILE + '_predictions.csv', converters={'season': str})\n",
    "    # WHO recommendation name, sequence\n",
    "    who_ha_df = pd.read_csv(WHO_DIR + FILE + '.csv')\n",
    "    who_na_df = pd.read_csv(WHO_DIR + FILE + '.csv')\n",
    "    df['name_who'] = who_ha_df['who_recommendation_name']\n",
    "    df['ha_seq_who'] = who_ha_df['ha_sequence']\n",
    "    df['na_seq_who'] = who_na_df['na_sequence']\n",
    "    \n",
    "    # Compute errors\n",
    "    seasons = df['season'].values\n",
    "    enet_errors_ha = []\n",
    "    enet_errors_na = []\n",
    "    who_errors_ha = []\n",
    "    who_errors_na = []\n",
    "    \n",
    "    for season in seasons:\n",
    "        season_str = str(season)\n",
    "        if len(season_str) == 1:\n",
    "            season_str = str('0' + season_str)\n",
    "            \n",
    "        # Read all sequences\n",
    "        DATA_DIR = 'data/merged/' + FILE + '/' + FILE + '_' + season_str + '.csv'\n",
    "        if not os.path.isfile(DATA_DIR):\n",
    "            enet_errors_ha.append(-1)\n",
    "            enet_errors_na.append(-1)\n",
    "            who_errors_ha.append(-1)\n",
    "            who_errors_na.append(-1)\n",
    "            continue\n",
    "        seq_df = pd.read_csv(DATA_DIR)\n",
    "        \n",
    "        # Access Enet and WHO recommendations\n",
    "        enet_ha_seq_0 = df[df['season'] == season]['ha_seq_0'].values[0][:HA_TRUNC]\n",
    "        enet_na_seq_0 = df[df['season'] == season]['na_seq_0'].values[0][:NA_TRUNC]\n",
    "        enet_ha_seq_1 = df[df['season'] == season]['ha_seq_1'].values[0][:HA_TRUNC]\n",
    "        enet_na_seq_1 = df[df['season'] == season]['na_seq_1'].values[0][:NA_TRUNC]\n",
    "        who_ha_seq = df[df['season'] == season]['ha_seq_who'].values[0][:HA_TRUNC]\n",
    "        who_na_seq = df[df['season'] == season]['na_seq_who'].values[0][:NA_TRUNC]\n",
    "        \n",
    "        # Find average enet and who errors\n",
    "        total_enet_error_ha = 0\n",
    "        total_enet_error_na = 0\n",
    "        total_who_error_ha = 0\n",
    "        total_who_error_na = 0\n",
    "        \n",
    "        # HA\n",
    "        ha_len = len(seq_df)\n",
    "        for seq in seq_df['sequence']:\n",
    "            if len(seq) < HA_TRUNC:\n",
    "                ha_len -= 1\n",
    "                continue\n",
    "            total_enet_error_ha += min(distance(enet_ha_seq_0, seq[:HA_TRUNC]), distance(enet_ha_seq_1, seq[:HA_TRUNC]))\n",
    "            total_who_error_ha += distance(who_ha_seq, seq[:HA_TRUNC])\n",
    "        # NA\n",
    "        na_len = len(seq_df)\n",
    "        for seq in seq_df['sequence_na']:\n",
    "            if len(seq) < NA_TRUNC:\n",
    "                na_len -= 1\n",
    "                continue\n",
    "            total_enet_error_na += min(distance(enet_na_seq_0, seq[:NA_TRUNC]), distance(enet_na_seq_1, seq[:NA_TRUNC]))\n",
    "            total_who_error_na += distance(who_na_seq, seq[:NA_TRUNC])\n",
    "            \n",
    "        enet_errors_ha.append(total_enet_error_ha/ha_len)\n",
    "        enet_errors_na.append(total_enet_error_na/na_len)\n",
    "        who_errors_ha.append(total_who_error_ha/ha_len)\n",
    "        who_errors_na.append(total_who_error_na/na_len)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df['ha_who_error'] = who_errors_ha\n",
    "    df['na_who_error'] = who_errors_na\n",
    "    df['ha_enet_error'] = enet_errors_ha\n",
    "    df['na_enet_error'] = enet_errors_na\n",
    "    df.to_csv(OUT_DIR + FILE + '.csv', index=False)\n",
    "    \n",
    "\n",
    "# Error differences between Enet and WHO\n",
    "# Positive means we are better, negative means WHO better\n",
    "north_errors_df = pd.DataFrame({'north_season':NORTH_YEARS})\n",
    "for FILE in FILES[:2]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    north_errors_df[FILE + '_ha'] = df['ha_who_error'] - df['ha_enet_error']\n",
    "    north_errors_df[FILE + '_na'] = df['na_who_error'] - df['na_enet_error']\n",
    "    \n",
    "south_errors_df = pd.DataFrame({'south_season':SOUTH_YEARS})\n",
    "for FILE in FILES[2:]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    south_errors_df[FILE + '_ha'] = df['ha_who_error'] - df['ha_enet_error']\n",
    "    south_errors_df[FILE + '_na'] = df['na_who_error'] - df['na_enet_error']\n",
    "    \n",
    "errors_df = north_errors_df.join(south_errors_df, how='outer')\n",
    "errors_df.to_csv(OUT_DIR + 'errors_difference.csv', index=False)\n",
    "print('Average Improvement')\n",
    "print(f'\\tNorth H1N1 HA: {np.mean(errors_df[\"north_h1n1_ha\"]):.3f}')\n",
    "print(f'\\tNorth H3N2 HA: {np.mean(errors_df[\"north_h3n2_ha\"]):.3f}')\n",
    "print(f'\\tSouth H1N1 HA: {np.mean(errors_df[\"south_h1n1_ha\"]):.3f}')\n",
    "print(f'\\tSouth H3N2 HA: {np.mean(errors_df[\"south_h3n2_ha\"]):.3f}')\n",
    "errors_df[['north_season', 'north_h1n1_ha', 'north_h3n2_ha', \n",
    "           'south_season', 'south_h1n1_ha', 'south_h3n2_ha']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c0017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat for plotdata in paper\n",
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '.csv')\n",
    "    df = df.rename(columns={'ha_who_error':'ldistance_WHO', 'ha_enet_error':'ldistance_Qnet_recommendation'})\n",
    "    df.to_csv(TAB_DIR + FILE + '_two.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee4399",
   "metadata": {},
   "source": [
    "### 2009-10 H1N1 Northern flu season\n",
    "\n",
    "Hamming distance between A/Hong Kong/H090-751-V3 (EPI326467) and [pdm09](https://www.ncbi.nlm.nih.gov/nuccore/NC_026433.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48d95952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdm09_ha_seq = 'MKAILVVLLYTFATANADTLCIGYHANNSTDTVDTVLEKNVTVTHSVNLLEDKHNGKLCKLRGVAPLHLGKCNIAGWILGNPECESLSTASSWSYIVETPSSDNGTCYPGDFIDYEELREQLSSVSSFERFEIFPKTSSWPNHDSNKGVTAACPHAGAKSFYKNLIWLVKKGNSYPKLSKSYINDKGKEVLVLWGIHHPSTSADQQSLYQNADAYVFVGSSRYSKKFKPEIAIRPKVRXXEGRMNYYWTLVEPGDKITFEATGNLVVPRYAFAMERNAGSGIIISDTPVHDCNTTCQTPKGAINTSLPFQNIHPITIGKCPKYVKSTKLRLATGLRNIPSIQSRGLFGAIAGFIEGGWTGMVDGWYGYHHQNEQGSGYAADLKSTQNAIDEITNKVNSVIEKMNTQFTAVGKEFNHLEKRIENLNKKVDDGFLDIWTYNAELLVLLENERTLDYHDSNVKNLYEKVRSQLKNNAKEIGNGCFEFYHKCDNTCMESVKNGTYDYPKYSEEAKLNREEIDGVKLESTRIYQILAIYSTVASSLVLVVSLGAISFWMCSNGSLQCRICI'\n",
    "north_h1n1 = pd.read_csv(OUT_DIR + 'north_h1n1.csv')\n",
    "hongkong = north_h1n1[north_h1n1['name_1']=='A/Hong_Kong/H090-751-V3']\n",
    "hongkong_ha_seq = hongkong['ha_seq_1'].values[0]\n",
    "distance(pdm09_ha_seq[:565], hongkong_ha_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe27bf6",
   "metadata": {},
   "source": [
    "## Single-Cluster Predictions vs. WHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af15a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Improvement\n",
      "\tNorth H1N1 HA: 3.380\n",
      "\tNorth H3N2 HA: 3.155\n",
      "\tSouth H1N1 HA: 3.784\n",
      "\tSouth H3N2 HA: 1.911\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>north_season</th>\n",
       "      <th>north_h1n1_ha</th>\n",
       "      <th>north_h3n2_ha</th>\n",
       "      <th>south_season</th>\n",
       "      <th>south_h1n1_ha</th>\n",
       "      <th>south_h3n2_ha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03_04</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>22.67</td>\n",
       "      <td>03</td>\n",
       "      <td>4.31</td>\n",
       "      <td>8.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04_05</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>2.94</td>\n",
       "      <td>04</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05_06</td>\n",
       "      <td>4.18</td>\n",
       "      <td>7.82</td>\n",
       "      <td>05</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06_07</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.36</td>\n",
       "      <td>06</td>\n",
       "      <td>2.35</td>\n",
       "      <td>8.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07_08</td>\n",
       "      <td>-3.02</td>\n",
       "      <td>7.84</td>\n",
       "      <td>07</td>\n",
       "      <td>6.65</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08_09</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.59</td>\n",
       "      <td>08</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>09_10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.96</td>\n",
       "      <td>09</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10_11</td>\n",
       "      <td>2.79</td>\n",
       "      <td>3.53</td>\n",
       "      <td>10</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11_12</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2.72</td>\n",
       "      <td>11</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12_13</td>\n",
       "      <td>7.28</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>12</td>\n",
       "      <td>5.72</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13_14</td>\n",
       "      <td>6.74</td>\n",
       "      <td>4.79</td>\n",
       "      <td>13</td>\n",
       "      <td>4.04</td>\n",
       "      <td>-4.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14_15</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>14</td>\n",
       "      <td>11.34</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15_16</td>\n",
       "      <td>9.87</td>\n",
       "      <td>5.09</td>\n",
       "      <td>15</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16_17</td>\n",
       "      <td>14.14</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>16</td>\n",
       "      <td>11.77</td>\n",
       "      <td>-0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17_18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>17</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18_19</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>18</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19_20</td>\n",
       "      <td>5.57</td>\n",
       "      <td>1.24</td>\n",
       "      <td>19</td>\n",
       "      <td>4.95</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20_21</td>\n",
       "      <td>5.65</td>\n",
       "      <td>-5.98</td>\n",
       "      <td>20</td>\n",
       "      <td>5.22</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21_22</td>\n",
       "      <td>2.40</td>\n",
       "      <td>8.80</td>\n",
       "      <td>21</td>\n",
       "      <td>0.57</td>\n",
       "      <td>5.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22_23</td>\n",
       "      <td>-3.80</td>\n",
       "      <td>1.65</td>\n",
       "      <td>22</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23_24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   north_season  north_h1n1_ha  north_h3n2_ha south_season  south_h1n1_ha  \\\n",
       "0         03_04          -3.00          22.67           03           4.31   \n",
       "1         04_05          -2.29           2.94           04           0.50   \n",
       "2         05_06           4.18           7.82           05           2.94   \n",
       "3         06_07           4.05           2.36           06           2.35   \n",
       "4         07_08          -3.02           7.84           07           6.65   \n",
       "5         08_09           0.40           2.59           08          -1.63   \n",
       "6         09_10           0.00           1.96           09          -0.07   \n",
       "7         10_11           2.79           3.53           10           3.71   \n",
       "8         11_12           5.17           2.72           11           3.75   \n",
       "9         12_13           7.28          -2.20           12           5.72   \n",
       "10        13_14           6.74           4.79           13           4.04   \n",
       "11        14_15           8.85           0.85           14          11.34   \n",
       "12        15_16           9.87           5.09           15          10.80   \n",
       "13        16_17          14.14          -0.85           16          11.77   \n",
       "14        17_18           1.00          -1.55           17           0.98   \n",
       "15        18_19           4.98          -0.00           18           2.97   \n",
       "16        19_20           5.57           1.24           19           4.95   \n",
       "17        20_21           5.65          -5.98           20           5.22   \n",
       "18        21_22           2.40           8.80           21           0.57   \n",
       "19        22_23          -3.80           1.65           22          -1.42   \n",
       "20        23_24           0.00           0.00           23           0.00   \n",
       "\n",
       "    south_h3n2_ha  \n",
       "0            8.04  \n",
       "1            2.05  \n",
       "2            2.63  \n",
       "3            8.96  \n",
       "4            3.66  \n",
       "5           -0.88  \n",
       "6            2.01  \n",
       "7            0.83  \n",
       "8            3.43  \n",
       "9            3.26  \n",
       "10          -4.70  \n",
       "11           2.10  \n",
       "12           0.87  \n",
       "13          -0.74  \n",
       "14          -0.96  \n",
       "15           0.78  \n",
       "16           1.94  \n",
       "17           1.04  \n",
       "18           5.44  \n",
       "19           0.38  \n",
       "20           0.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for FILE in FILES:    \n",
    "    # Prepare prediction DataFrame\n",
    "    df = pd.read_csv(PRED_DIR + FILE + '_predictions_single_cluster.csv', converters={'season': str})\n",
    "    # WHO recommendation name, sequence\n",
    "    who_ha_df = pd.read_csv(WHO_DIR + FILE + '.csv')\n",
    "    who_na_df = pd.read_csv(WHO_DIR + FILE + '.csv')\n",
    "    df['name_who'] = who_ha_df['who_recommendation_name']\n",
    "    df['ha_seq_who'] = who_ha_df['ha_sequence']\n",
    "    df['na_seq_who'] = who_na_df['na_sequence']\n",
    "    \n",
    "    # Compute errors\n",
    "    seasons = df['season'].values\n",
    "    enet_errors_ha = []\n",
    "    enet_errors_na = []\n",
    "    who_errors_ha = []\n",
    "    who_errors_na = []\n",
    "    \n",
    "    for season in seasons:\n",
    "        season_str = str(season)\n",
    "        if len(season_str) == 1:\n",
    "            season_str = str('0' + season_str)\n",
    "            \n",
    "        # Read all sequences\n",
    "        DATA_DIR = 'data/merged/' + FILE + '/' + FILE + '_' + season_str + '.csv'\n",
    "        if not os.path.isfile(DATA_DIR):\n",
    "            enet_errors_ha.append(-1)\n",
    "            enet_errors_na.append(-1)\n",
    "            who_errors_ha.append(-1)\n",
    "            who_errors_na.append(-1)\n",
    "            continue\n",
    "        seq_df = pd.read_csv(DATA_DIR)\n",
    "        \n",
    "        # Access Enet and WHO recommendations\n",
    "        enet_ha_seq = df[df['season'] == season]['ha_seq'].values[0][:HA_TRUNC]\n",
    "        enet_na_seq = df[df['season'] == season]['na_seq'].values[0][:NA_TRUNC]\n",
    "        who_ha_seq = df[df['season'] == season]['ha_seq_who'].values[0][:HA_TRUNC]\n",
    "        who_na_seq = df[df['season'] == season]['na_seq_who'].values[0][:NA_TRUNC]\n",
    "        \n",
    "        # Find average enet and who errors\n",
    "        total_enet_error_ha = 0\n",
    "        total_enet_error_na = 0\n",
    "        total_who_error_ha = 0\n",
    "        total_who_error_na = 0\n",
    "        \n",
    "        # HA\n",
    "        ha_len = len(seq_df)\n",
    "        for seq in seq_df['sequence']:\n",
    "            if len(seq) < HA_TRUNC:\n",
    "                ha_len -= 1\n",
    "                continue\n",
    "            total_enet_error_ha += distance(enet_ha_seq, seq[:HA_TRUNC])\n",
    "            total_who_error_ha += distance(who_ha_seq, seq[:HA_TRUNC])\n",
    "        # NA\n",
    "        na_len = len(seq_df)\n",
    "        for seq in seq_df['sequence_na']:\n",
    "            if len(seq) < NA_TRUNC:\n",
    "                na_len -= 1\n",
    "                continue\n",
    "            total_enet_error_na += distance(enet_na_seq, seq[:NA_TRUNC])\n",
    "            total_who_error_na += distance(who_na_seq, seq[:NA_TRUNC])\n",
    "            \n",
    "        enet_errors_ha.append(total_enet_error_ha/ha_len)\n",
    "        enet_errors_na.append(total_enet_error_na/na_len)\n",
    "        who_errors_ha.append(total_who_error_ha/ha_len)\n",
    "        who_errors_na.append(total_who_error_na/na_len)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df['ha_who_error'] = who_errors_ha\n",
    "    df['na_who_error'] = who_errors_na\n",
    "    df['ha_enet_error'] = enet_errors_ha\n",
    "    df['na_enet_error'] = enet_errors_na\n",
    "    df.to_csv(OUT_DIR + FILE + '_single_cluster.csv', index=False)\n",
    "    \n",
    "\n",
    "# Error differences between Enet and WHO\n",
    "# Positive means we are better, negative means WHO better\n",
    "north_errors_df = pd.DataFrame({'north_season':NORTH_YEARS})\n",
    "for FILE in FILES[:2]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_single_cluster.csv')\n",
    "    north_errors_df[FILE + '_ha'] = df['ha_who_error'] - df['ha_enet_error']\n",
    "    north_errors_df[FILE + '_na'] = df['na_who_error'] - df['na_enet_error']\n",
    "    \n",
    "south_errors_df = pd.DataFrame({'south_season':SOUTH_YEARS})\n",
    "for FILE in FILES[2:]:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_single_cluster.csv')\n",
    "    south_errors_df[FILE + '_ha'] = df['ha_who_error'] - df['ha_enet_error']\n",
    "    south_errors_df[FILE + '_na'] = df['na_who_error'] - df['na_enet_error']\n",
    "    \n",
    "errors_df = north_errors_df.join(south_errors_df, how='outer')\n",
    "errors_df.to_csv(OUT_DIR + 'errors_difference_single_cluster.csv', index=False)\n",
    "print('Average Improvement')\n",
    "print(f'\\tNorth H1N1 HA: {np.mean(errors_df[\"north_h1n1_ha\"]):.3f}')\n",
    "print(f'\\tNorth H3N2 HA: {np.mean(errors_df[\"north_h3n2_ha\"]):.3f}')\n",
    "print(f'\\tSouth H1N1 HA: {np.mean(errors_df[\"south_h1n1_ha\"]):.3f}')\n",
    "print(f'\\tSouth H3N2 HA: {np.mean(errors_df[\"south_h3n2_ha\"]):.3f}')\n",
    "errors_df[['north_season', 'north_h1n1_ha', 'north_h3n2_ha', \n",
    "           'south_season', 'south_h1n1_ha', 'south_h3n2_ha']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de9184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat for plotdata in paper\n",
    "for FILE in FILES:\n",
    "    df = pd.read_csv(OUT_DIR + FILE + '_single_cluster.csv')\n",
    "    df = df.rename(columns={'ha_who_error':'ldistance_WHO', 'ha_enet_error':'ldistance_Qnet_recommendation'})\n",
    "    df.to_csv(TAB_DIR + FILE + '_single.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78efc808",
   "metadata": {},
   "source": [
    "## Comparison to Huddleston et al. 2020\n",
    "\n",
    "https://elifesciences.org/articles/60067#fig11s1\n",
    "\n",
    "Interesting points:\n",
    "- They take the average of the 12 points (validation and test, not just test)\n",
    "- They use a combination of north and south timepoints (mostly South)\n",
    "- For WHO sequences used across multiple years, they only evaluate the earliest one\n",
    "    - Ex. A/California/7/2004 is used for the vaccine for north 2005-06 and south 2006, so the timepoint they chose was north 2005-06 since it was earlier\n",
    "    - Ex. A/Perth/16/2009 is used for south 2010, north 2010-11, south 2011, north 2011-12, south 2012, so the timepoint they chose was south 2010 since it is the earliest\n",
    "- We do best with using multicluster predictions, then making a single prediction using the cluster counts (rather than areas) of the two largest clusters\n",
    "\n",
    "The averages below match the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1581dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -3.825\n",
      "Naive: -2.193\n",
      "LBI: -1.927\n",
      "HI: -2.333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strain_type</th>\n",
       "      <th>strain</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>distance_to_future</th>\n",
       "      <th>relative_distance_to_future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Fujian/411/2002</td>\n",
       "      <td>FU02</td>\n",
       "      <td>2003-10-01</td>\n",
       "      <td>9.44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Wellington/1/2004</td>\n",
       "      <td>WE04</td>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>6.08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/California/7/2004</td>\n",
       "      <td>CA04</td>\n",
       "      <td>2005-04-01</td>\n",
       "      <td>7.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Wisconsin/67/2005</td>\n",
       "      <td>WI05</td>\n",
       "      <td>2006-04-01</td>\n",
       "      <td>10.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Brisbane/10/2007</td>\n",
       "      <td>BR07</td>\n",
       "      <td>2007-10-01</td>\n",
       "      <td>5.78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Perth/16/2009</td>\n",
       "      <td>PE09</td>\n",
       "      <td>2009-10-01</td>\n",
       "      <td>7.95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Victoria/361/2011</td>\n",
       "      <td>VI11</td>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>6.27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Texas/50/2012</td>\n",
       "      <td>TX12</td>\n",
       "      <td>2013-10-01</td>\n",
       "      <td>8.65</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Switzerland/9715293/2013</td>\n",
       "      <td>SW13</td>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>10.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/HongKong/4801/2014</td>\n",
       "      <td>HK14</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>6.09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Singapore/Infimh-16-0019/2016</td>\n",
       "      <td>SI16</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>8.61</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>A/Switzerland/8060/2017</td>\n",
       "      <td>SW17</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>15.17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   strain_type                           strain abbreviation   timepoint  \\\n",
       "0      vaccine                A/Fujian/411/2002         FU02  2003-10-01   \n",
       "1      vaccine              A/Wellington/1/2004         WE04  2004-10-01   \n",
       "2      vaccine              A/California/7/2004         CA04  2005-04-01   \n",
       "3      vaccine              A/Wisconsin/67/2005         WI05  2006-04-01   \n",
       "4      vaccine               A/Brisbane/10/2007         BR07  2007-10-01   \n",
       "5      vaccine                  A/Perth/16/2009         PE09  2009-10-01   \n",
       "6      vaccine              A/Victoria/361/2011         VI11  2012-04-01   \n",
       "7      vaccine                  A/Texas/50/2012         TX12  2013-10-01   \n",
       "8      vaccine       A/Switzerland/9715293/2013         SW13  2014-10-01   \n",
       "9      vaccine             A/HongKong/4801/2014         HK14  2015-10-01   \n",
       "10     vaccine  A/Singapore/Infimh-16-0019/2016         SI16  2017-10-01   \n",
       "11     vaccine          A/Switzerland/8060/2017         SW17  2018-10-01   \n",
       "\n",
       "    distance_to_future  relative_distance_to_future  \n",
       "0                 9.44                          NaN  \n",
       "1                 6.08                          NaN  \n",
       "2                 7.50                          NaN  \n",
       "3                10.30                          NaN  \n",
       "4                 5.78                          NaN  \n",
       "5                 7.95                          NaN  \n",
       "6                 6.27                          NaN  \n",
       "7                 8.65                          NaN  \n",
       "8                10.82                          NaN  \n",
       "9                 6.09                          NaN  \n",
       "10                8.61                          NaN  \n",
       "11               15.17                          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huddleston = pd.read_csv('data/huddleston.csv')\n",
    "print(f'Best: {huddleston[12:24][\"relative_distance_to_future\"].mean():.3f}')\n",
    "print(f'Naive: {huddleston[24:36][\"relative_distance_to_future\"].mean():.3f}')\n",
    "print(f'LBI: {huddleston[36:48][\"relative_distance_to_future\"].mean():.3f}')\n",
    "print(f'HI: {huddleston[48:60][\"relative_distance_to_future\"].mean():.3f}')\n",
    "huddleston[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f4568bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WHO Recommendation</th>\n",
       "      <th>Emergenet Distance to Future</th>\n",
       "      <th>Emergenet Improvement</th>\n",
       "      <th>Mutational Load + LBI Distance to Future</th>\n",
       "      <th>Mutational Load + LBI Improvement</th>\n",
       "      <th>HI + Mutational Load Distance to Future</th>\n",
       "      <th>HI + Mutational Load Improvement</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timepoint</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-10-01</th>\n",
       "      <td>A/Fujian/411/2002</td>\n",
       "      <td>7.28</td>\n",
       "      <td>2.05</td>\n",
       "      <td>8.44</td>\n",
       "      <td>1.01</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-10-01</th>\n",
       "      <td>A/Wellington/1/2004</td>\n",
       "      <td>4.06</td>\n",
       "      <td>2.63</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.70</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-01</th>\n",
       "      <td>A/California/7/2004</td>\n",
       "      <td>5.18</td>\n",
       "      <td>7.82</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.89</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-04-01</th>\n",
       "      <td>A/Wisconsin/67/2005</td>\n",
       "      <td>8.54</td>\n",
       "      <td>2.36</td>\n",
       "      <td>5.36</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.37</td>\n",
       "      <td>4.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-10-01</th>\n",
       "      <td>A/Brisbane/10/2007</td>\n",
       "      <td>5.78</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-01</th>\n",
       "      <td>A/Perth/16/2009</td>\n",
       "      <td>8.03</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.93</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-01</th>\n",
       "      <td>A/Victoria/361/2011</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.79</td>\n",
       "      <td>3.72</td>\n",
       "      <td>2.54</td>\n",
       "      <td>7.02</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-01</th>\n",
       "      <td>A/Texas/50/2012</td>\n",
       "      <td>6.66</td>\n",
       "      <td>2.10</td>\n",
       "      <td>6.54</td>\n",
       "      <td>2.11</td>\n",
       "      <td>6.89</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-01</th>\n",
       "      <td>A/Switzerland/9715293/2013</td>\n",
       "      <td>9.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3.88</td>\n",
       "      <td>6.94</td>\n",
       "      <td>3.88</td>\n",
       "      <td>6.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-01</th>\n",
       "      <td>A/Hong Kong/4801/2014</td>\n",
       "      <td>7.64</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>7.33</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>6.09</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>A/Singapore/INFIMH-16-0019/2016</td>\n",
       "      <td>8.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>7.47</td>\n",
       "      <td>1.15</td>\n",
       "      <td>7.81</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>A/Switzerland/8060/2017</td>\n",
       "      <td>13.97</td>\n",
       "      <td>1.94</td>\n",
       "      <td>17.08</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>10.42</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.48</td>\n",
       "      <td>2.05</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1.93</td>\n",
       "      <td>6.22</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         WHO Recommendation  Emergenet Distance to Future  \\\n",
       "Timepoint                                                                   \n",
       "2003-10-01                A/Fujian/411/2002                          7.28   \n",
       "2004-10-01              A/Wellington/1/2004                          4.06   \n",
       "2005-04-01              A/California/7/2004                          5.18   \n",
       "2006-04-01              A/Wisconsin/67/2005                          8.54   \n",
       "2007-10-01               A/Brisbane/10/2007                          5.78   \n",
       "2009-10-01                  A/Perth/16/2009                          8.03   \n",
       "2012-04-01              A/Victoria/361/2011                          4.00   \n",
       "2013-10-01                  A/Texas/50/2012                          6.66   \n",
       "2014-10-01       A/Switzerland/9715293/2013                          9.85   \n",
       "2015-10-01            A/Hong Kong/4801/2014                          7.64   \n",
       "2017-10-01  A/Singapore/INFIMH-16-0019/2016                          8.79   \n",
       "2018-10-01          A/Switzerland/8060/2017                         13.97   \n",
       "NaN                                     NaN                          7.48   \n",
       "\n",
       "            Emergenet Improvement  Mutational Load + LBI Distance to Future  \\\n",
       "Timepoint                                                                     \n",
       "2003-10-01                   2.05                                      8.44   \n",
       "2004-10-01                   2.63                                      4.38   \n",
       "2005-04-01                   7.82                                      4.60   \n",
       "2006-04-01                   2.36                                      5.36   \n",
       "2007-10-01                  -0.88                                      3.78   \n",
       "2009-10-01                   0.83                                      6.95   \n",
       "2012-04-01                   4.79                                      3.72   \n",
       "2013-10-01                   2.10                                      6.54   \n",
       "2014-10-01                   0.87                                      3.88   \n",
       "2015-10-01                  -0.74                                      7.33   \n",
       "2017-10-01                   0.78                                      7.47   \n",
       "2018-10-01                   1.94                                     17.08   \n",
       "NaN                          2.05                                      6.63   \n",
       "\n",
       "            Mutational Load + LBI Improvement  \\\n",
       "Timepoint                                       \n",
       "2003-10-01                               1.01   \n",
       "2004-10-01                               1.70   \n",
       "2005-04-01                               2.89   \n",
       "2006-04-01                               4.94   \n",
       "2007-10-01                               2.00   \n",
       "2009-10-01                               1.00   \n",
       "2012-04-01                               2.54   \n",
       "2013-10-01                               2.11   \n",
       "2014-10-01                               6.94   \n",
       "2015-10-01                              -1.24   \n",
       "2017-10-01                               1.15   \n",
       "2018-10-01                              -1.91   \n",
       "NaN                                      1.93   \n",
       "\n",
       "            HI + Mutational Load Distance to Future  \\\n",
       "Timepoint                                             \n",
       "2003-10-01                                     6.50   \n",
       "2004-10-01                                     4.38   \n",
       "2005-04-01                                     4.60   \n",
       "2006-04-01                                     5.37   \n",
       "2007-10-01                                     3.78   \n",
       "2009-10-01                                     7.93   \n",
       "2012-04-01                                     7.02   \n",
       "2013-10-01                                     6.89   \n",
       "2014-10-01                                     3.88   \n",
       "2015-10-01                                     6.09   \n",
       "2017-10-01                                     7.81   \n",
       "2018-10-01                                    10.42   \n",
       "NaN                                            6.22   \n",
       "\n",
       "            HI + Mutational Load Improvement  \n",
       "Timepoint                                     \n",
       "2003-10-01                              2.95  \n",
       "2004-10-01                              1.70  \n",
       "2005-04-01                              2.89  \n",
       "2006-04-01                              4.94  \n",
       "2007-10-01                              2.00  \n",
       "2009-10-01                              0.02  \n",
       "2012-04-01                             -0.75  \n",
       "2013-10-01                              1.76  \n",
       "2014-10-01                              6.94  \n",
       "2015-10-01                              0.00  \n",
       "2017-10-01                              0.80  \n",
       "2018-10-01                              4.75  \n",
       "NaN                                     2.33  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "south = pd.read_csv(OUT_DIR + 'south_h3n2_single_cluster.csv', converters={'season': str})[['season','name','name_who','ha_who_error','ha_enet_error']].iloc[[1,2,5,7,11,12,13,15,16]]\n",
    "north = pd.read_csv(OUT_DIR + 'north_h3n2_single_cluster.csv', converters={'season': str})[['season','name','name_who','ha_who_error','ha_enet_error']].iloc[[2,3,10]]\n",
    "enet = south.append(north)\n",
    "\n",
    "new_order = np.array([1, 2, 10, 11, 3, 4, 12, 5, 6, 7, 8, 9])\n",
    "enet = enet.iloc[new_order - 1].reset_index(drop=True)\n",
    "enet['enet_improvement'] = enet['ha_who_error'] - enet['ha_enet_error']\n",
    "\n",
    "# Make table for paper\n",
    "enet['Timepoint'] = huddleston[:12]['timepoint']\n",
    "enet['Mutational Load + LBI Distance to Future'] = huddleston[36:48]['distance_to_future'].values\n",
    "enet['Mutational Load + LBI Improvement'] = -huddleston[36:48]['relative_distance_to_future'].values\n",
    "enet['HI + Mutational Load Distance to Future'] = huddleston[48:60]['distance_to_future'].values\n",
    "enet['HI + Mutational Load Improvement'] = -huddleston[48:60]['relative_distance_to_future'].values\n",
    "enet = enet.rename(columns={'name_who':'WHO Recommendation', \n",
    "                            'name':'Enet Recommendation',\n",
    "                            'ha_enet_error':'Emergenet Distance to Future',\n",
    "                            'enet_improvement':'Emergenet Improvement'})\n",
    "enet = enet[['Timepoint', 'WHO Recommendation',\n",
    "             'Emergenet Distance to Future', 'Emergenet Improvement',\n",
    "             'Mutational Load + LBI Distance to Future', 'Mutational Load + LBI Improvement', \n",
    "             'HI + Mutational Load Distance to Future', 'HI + Mutational Load Improvement']].round(2)\n",
    "enet = enet.append(enet.mean(), ignore_index=True).round(2)\n",
    "enet.to_csv(OUT_DIR+'huddleston_comparison.csv', index=False)\n",
    "enet = enet.set_index('Timepoint')\n",
    "textable(enet, tabname = 'tables/huddleston_comparison.tex', FORMAT='%1.2f')\n",
    "enet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
