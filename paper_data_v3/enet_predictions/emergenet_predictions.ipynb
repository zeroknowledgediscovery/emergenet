{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c4fdeb",
   "metadata": {},
   "source": [
    "# Influenza Enet Predictions\n",
    "\n",
    "Predicting dominant strains using Emergenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044273eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from domseq import DomSeq\n",
    "\n",
    "PRED_DIR = 'results/enet_predictions/seasonal_predictions/'\n",
    "OUT_DIR = 'results/enet_predictions/'\n",
    "\n",
    "FILES = ['north_h1n1', 'north_h3n2', 'south_h1n1', 'south_h3n2']\n",
    "\n",
    "NORTH_YEARS = []\n",
    "for i in np.arange(3, 24):\n",
    "    YEAR = ''\n",
    "    if i < 10:\n",
    "        YEAR += '0' + str(i)\n",
    "    else:\n",
    "        YEAR += (str(i))\n",
    "    if i + 1 < 10:\n",
    "        YEAR += '_0' + str(i + 1)\n",
    "    else:\n",
    "        YEAR += '_' + str(i + 1)\n",
    "    NORTH_YEARS.append(YEAR)\n",
    "        \n",
    "SOUTH_YEARS = []\n",
    "for i in np.arange(3, 24):\n",
    "    if i < 10:\n",
    "        SOUTH_YEARS.append('0' + str(i))\n",
    "    else:\n",
    "        SOUTH_YEARS.append(str(i))\n",
    "\n",
    "NA_TRUNC = 468 # 2 less than official length of 470\n",
    "HA_TRUNC = 565 # 2 less than official length of 567"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a82805",
   "metadata": {},
   "source": [
    "## Create Enet Models\n",
    "- Truncate HA at 565 amino acids, we only need HA models\n",
    "- Give Enet the same name as the data file\n",
    "    \n",
    "### Running Processes\n",
    "\n",
    "Computations are done in:\n",
    "- `enet_train.py`\n",
    "- `run_enet_train.sh`\n",
    "\n",
    "To run, navigate to terminal and do\n",
    "\n",
    "`chmod ugo+rwx run_enet_train.sh`\n",
    "\n",
    "`./run_enet_train.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3de2e",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "E-Centroid: $$x_{*}^{t+\\delta} = argmin_{y\\in \\bigcup_{r\\leq t}H^{\\tau}} \\left ( \\sum_{x \\in {H^t}} \\theta(x,y) - |H^t|A \\ln\\omega_y \\right )$$\n",
    "- $x_{*}^{t+\\delta}$ is the dominant strain in the upcoming flu season at time $t+\\omega$\n",
    "- $H^t$ is the sequence population at time $t$\n",
    "- $\\theta(x,y)$ is the e-distance between $x$ and $y$ in their respective Enets\n",
    "- $A = \\frac{1-\\alpha}{\\sqrt{8}N^2}$, where $\\alpha$ is a fixed significance level and $N$ is the sequence length considered\n",
    "- $\\ln\\omega_y$ is the membership degree of sequence $y$\n",
    "- **Predict dominant strain based on HA data** \n",
    "    \n",
    "Predictions:\n",
    "- Multi-cluster predictions: take the E-centroid of the two largest clusters\n",
    "- Single-cluster predictions: find the sequence who's distance to the two-cluster predictions is closest to the ratio of their clusters' respective sizes\n",
    "\n",
    "### Running Processes\n",
    "\n",
    "Computations are done in:\n",
    "- `enet_predictions.py`\n",
    "- `run_enet_predictions.sh`\n",
    "\n",
    "To run, navigate to terminal and do\n",
    "\n",
    "`chmod ugo+rwx run_enet_predictions.sh`\n",
    "\n",
    "`./run_enet_predictions.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd357922",
   "metadata": {},
   "source": [
    "## Aggregate Two-Cluster and Single-Cluster Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8ddd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "domseq = DomSeq(seq_trunc_length=HA_TRUNC, random_state=42)\n",
    "for FILE in FILES:\n",
    "    pred_df = pd.DataFrame(columns=['season',\n",
    "                                    'name_0','cluster_count_0','cluster_area_0',\n",
    "                                    'ha_acc_0','ha_seq_0','na_acc_0','na_seq_0',\n",
    "                                    'name_1','cluster_count_1','cluster_area_1',\n",
    "                                    'ha_acc_1','ha_seq_1','na_acc_1','na_seq_1']) \n",
    "    pred_df_single = pd.DataFrame(columns=['season','name',\n",
    "                                           'ha_acc','ha_seq',\n",
    "                                           'na_acc','na_seq']) \n",
    "    YEARS = []\n",
    "    if FILE[:5] == 'north':\n",
    "        YEARS = NORTH_YEARS\n",
    "        prev_season = '02_03'\n",
    "    else:\n",
    "        YEARS = SOUTH_YEARS\n",
    "        prev_season = '02'\n",
    "    for i in range(21):\n",
    "        pred_seqs = pd.read_csv(PRED_DIR + FILE + '/' + FILE + '_' + YEARS[i] + '.csv')\n",
    "        pred_seqs = pred_seqs.sort_values(by='cluster_count', ascending=False)\n",
    "        \n",
    "        # Expand multicluster predictions to larger dataframe\n",
    "        df1 = pd.DataFrame({'season':[YEARS[i]]})\n",
    "        for j in range(2):\n",
    "            df1['name_'+str(j)] = pred_seqs['name'].values[j]\n",
    "            df1['cluster_count_'+str(j)] = pred_seqs['cluster_count'].values[j]\n",
    "            df1['cluster_area_'+str(j)] = pred_seqs['cluster_area'].values[j]\n",
    "            df1['ha_acc_'+str(j)] = pred_seqs['acc'].values[j]\n",
    "            df1['ha_seq_'+str(j)] = pred_seqs['sequence'].values[j]\n",
    "            df1['na_acc_'+str(j)] = pred_seqs['acc_na'].values[j]\n",
    "            df1['na_seq_'+str(j)] = pred_seqs['sequence_na'].values[j]\n",
    "        pred_df = pd.concat([pred_df, df1])\n",
    "        \n",
    "        # Get single prediction\n",
    "        pred_seq_df = pd.read_csv('data/merged/' + FILE + '/pred/' + FILE + '_' + prev_season + '.csv')\n",
    "        prev_season = YEARS[i]\n",
    "        single_pred_seq = domseq.predict_single_domseq(pred_seqs, pred_seq_df)\n",
    "            \n",
    "        # Expand single cluster predictions to larger dataframe\n",
    "        df2 = pd.DataFrame({'season':[YEARS[i]]})\n",
    "        df2['name'] = single_pred_seq['name'].values[0]\n",
    "        df2['ha_acc'] = single_pred_seq['acc'].values[0]\n",
    "        df2['ha_seq'] = single_pred_seq['sequence'].values[0]\n",
    "        df2['na_acc'] = single_pred_seq['acc_na'].values[0]\n",
    "        df2['na_seq'] = single_pred_seq['sequence_na'].values[0]\n",
    "        pred_df_single = pd.concat([pred_df_single, df2])\n",
    "        \n",
    "    # Enet recommendation accession, name, sequence\n",
    "    pred_df.to_csv(OUT_DIR + FILE + '_predictions.csv', index=False)\n",
    "    pred_df_single.to_csv(OUT_DIR + FILE + '_predictions_single_cluster.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
