{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "threshold=6  # draw tree above this emergenec risk threshold\n",
    "CONSTRUCT_PHYLO=False  # use constructed tree (tree construction takes hours)\n",
    "OUTPUT_DIR='./'\n",
    "PHYLO_TREE_DIR='./'\n",
    "PHYLO_DIR='./'\n",
    "#COMBINED_RESULTS='../../../../paper_data/irat_enet/results/animal_predictions/combined_results.csv'\n",
    "COMBINED_RESULTS='/home/ishanu/Downloads/combined_results_irat.csv'\n",
    "#CONSTRUCT_PHYLO=False\n",
    "num_collapsed=19  # number of mutations within which leaves are collapsed\n",
    "VERBOSE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpf=pd.read_csv(COMBINED_RESULTS)\n",
    "IRATSEQ=tmpf[tmpf.is_irat==1].id.values.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class definition to hold multi sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqInfo(object):\n",
    "    \"\"\"Holds information regarding the sequence.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, seq, \n",
    "                 protein,\n",
    "                 accession,\n",
    "                 subtype=None,\n",
    "                 id=None,\n",
    "                 name=None,\n",
    "                 host=None, \n",
    "                 date=None, \n",
    "                 erisk=None,\n",
    "                 irisk=None,\n",
    "                 risk_flag=None,\n",
    "                 country=None):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.protein=protein\n",
    "        self.subtype=subtype        \n",
    "        self.seq = seq\n",
    "        self.accession = accession \n",
    "        self.host = host\n",
    "        self.date = date\n",
    "        self.erisk = erisk\n",
    "        self.irisk = irisk\n",
    "        self.risk_flag = risk_flag\n",
    "        self.country = country\n",
    "        \n",
    "class MultipleSeqInfo(object):\n",
    "    \"\"\"Holds information regarding multiple sequences.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): list of records parsed from NCBI\n",
    "        accessionname (str): column name for accession id\n",
    "        proteinname (str): protein name \n",
    "        risk_threshold (float): emergence risk threshold to compute distance matrix\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataframe,\n",
    "                 accessionname,\n",
    "                 proteinname,\n",
    "                 risk_threshold=6.2):\n",
    "        \n",
    "        self.seq_infos = {}\n",
    "        self.risk_threshold = risk_threshold\n",
    "        for i in np.arange(dataframe.index.size):\n",
    "            record=dataframe.iloc[i,:]\n",
    "            seqinfo = SeqInfo(\n",
    "                name=record.id,\n",
    "                seq=record[proteinname], \n",
    "                protein=proteinname,\n",
    "                accession=record[accessionname],\n",
    "                subtype=record.subtype,\n",
    "                erisk=record.emergence_risk,\n",
    "                irisk=record.impact_risk,\n",
    "                risk_flag = record.emergence_risk > self.risk_threshold,\n",
    "                host=None,\n",
    "                date=None,\n",
    "                country=None)\n",
    "            #print(record.predicted_emergence_score > self.risk_threshold)\n",
    "            self.seq_infos[seqinfo.accession] = seqinfo\n",
    "            \n",
    "    \n",
    "    def compute_L_distance_matrix(self):\n",
    "        highriskseq = pd.DataFrame.from_dict({key:val.seq \n",
    "                                              for (key,val) in self.seq_infos.items() \n",
    "                                              if val.risk_flag},orient='index',columns=['seq'])\n",
    "        num=highriskseq.index.size\n",
    "        d=np.zeros([num,num])\n",
    "        for x in tqdm(np.arange(num*num)):\n",
    "            j=x//num\n",
    "            i=x-num*j\n",
    "            if i > j:\n",
    "                d[i,j] = Levenshtein.distance(highriskseq.seq.values[i],\n",
    "                                                  highriskseq.seq.values[j])\n",
    "        ds=pd.DataFrame(d)        \n",
    "        ds=(ds+ds.transpose())\n",
    "        ds.columns=highriskseq.index.values\n",
    "        self.highriskdistancematrix=ds.copy()\n",
    "        \n",
    "        self.highriskdistancematrix.to_csv('dm'+str(self.risk_threshold)+'.csv',index=None)\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def accessions_to_subtype(self, accessions):\n",
    "        \"\"\"Create a dictionary mapping the accession to the host.\n",
    "        \"\"\"\n",
    "        \n",
    "        subtypes = []\n",
    "        for accession in accessions:\n",
    "            seqinfo = self.seq_infos[accession]\n",
    "            subtypes.append(seqinfo.subtype)\n",
    "            \n",
    "        return subtypes\n",
    "\n",
    "    def accessions_to_host(self, accessions):\n",
    "        \"\"\"Create a dictionary mapping the accession to the host.\n",
    "        \"\"\"\n",
    "        \n",
    "        hosts = []\n",
    "        for accession in accessions:\n",
    "            seqinfo = self.seq_infos[accession]\n",
    "            hosts.append(seqinfo.host)\n",
    "        return hosts\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(COMBINED_RESULTS,index_col=0).reset_index()\n",
    "ALLinfoHA=MultipleSeqInfo(df.reset_index(),'ha_accession','ha',risk_threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subtype\n",
       "H1N1    896\n",
       "H3N2    822\n",
       "H9N2     37\n",
       "H7N9      6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.emergence_risk>threshold].subtype.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# generate distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 3101121/3101121 [00:57<00:00, 53542.80it/s]\n"
     ]
    }
   ],
   "source": [
    "ALLinfoHA.compute_L_distance_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tree construction from distance matrix using biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Phylo import TreeConstruction\n",
    "from Bio import Phylo\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "def load_dm(file_, upper_diag=True):\n",
    "    \"\"\"Load the distance matrix. \n",
    "    \n",
    "    Also, do some preprocessing. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file_)\n",
    "    #df.set_index('Unnamed: 0', inplace=True)\n",
    "    #assert np.all(df.columns == df.index)\n",
    "    \n",
    "    # drop duplicate columns after reading csv\n",
    "    #df = df.loc[:, ~df.columns.str.replace(\"(\\.\\d+)$\", \"\").duplicated()]\n",
    "    \n",
    "    if upper_diag:\n",
    "        df = df + df.T\n",
    "    return df\n",
    "\n",
    "def save_tree(tree, file_name, save_type='xml'):\n",
    "    \"\"\"Saved the created phylogenetic tree.\"\"\"\n",
    "    \n",
    "    if save_type == 'pickle':\n",
    "        graph = Phylo.to_networkx(tree)\n",
    "        save_pickled(graph, file_name)\n",
    "    elif save_type == 'xml':\n",
    "        Phylo.write(tree, file_name, 'phyloxml')\n",
    "    else:\n",
    "        raise ValueError('Not a correct save type.')\n",
    "    \n",
    "def pandas_dm_to_biopython_dm(dm):\n",
    "    \"\"\"Convert the pandas distance matrix to the biopython distance matrix.\n",
    "    \n",
    "    Returns:\n",
    "        biopython distance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    accessions = dm.columns\n",
    "    bio_dm = []\n",
    "    for i, accession in enumerate(accessions):\n",
    "        bio_dm.append(list(dm.iloc[i, :i+1].values))\n",
    "        \n",
    "    bio_dm = TreeConstruction._DistanceMatrix(\n",
    "        list(dm.columns), \n",
    "        bio_dm)\n",
    "    \n",
    "    return bio_dm\n",
    "\n",
    "def distance_matrix_to_phylo_tree(dm, outfile=None):\n",
    "    \"\"\"Create a phylogenetic tree from the distance matrix.\"\"\"\n",
    "    \n",
    "    dm = pandas_dm_to_biopython_dm(dm)\n",
    "    \n",
    "    treeConstructor = TreeConstruction.DistanceTreeConstructor()\n",
    "    tree = treeConstructor.nj(dm)\n",
    "    \n",
    "    if outfile is not None:\n",
    "        save_tree(tree, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ete3 function, not all of these are used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree, TreeStyle\n",
    "from ete3 import Phyloxml\n",
    "from ete3 import AttrFace, faces, Tree, NodeStyle, TreeStyle\n",
    "\n",
    "def load_pickled(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f, encoding='latin')\n",
    "\n",
    "\n",
    "def get_farthest_node(tree, sequence):\n",
    "    return (tree&sequence).get_farthest_node()\n",
    "\n",
    "def get_all_accessions_from_tree(tree):\n",
    "    return [leaf_node.name for leaf_node in tree.get_leaves()]\n",
    "\n",
    "def remove_certain_hosts_from_tree(tree, hosts):\n",
    "    \"\"\"Remove leaf nodes if the host of that leaf is in `hosts`\"\"\"\n",
    "    \n",
    "    tree = copy.deepcopy(tree)\n",
    "    \n",
    "    removed_accessions = []\n",
    "    for leaf_node in tree.get_leaves():\n",
    "        if leaf_node.host in hosts:\n",
    "            leaf_node.detach()\n",
    "            \n",
    "    return tree\n",
    "\n",
    "def set_midpoint_outgroup(tree):\n",
    "    tree.set_outgroup(tree.get_midpoint_outgroup())\n",
    "\n",
    "\n",
    "def load_tree(filename, type_='phyloxml'):\n",
    "    \"\"\"Load saved phylogenetic tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    if type_ == 'phyloxml':\n",
    "        project = Phyloxml()\n",
    "        project.build_from_file(filename)\n",
    "\n",
    "        for tree in project.get_phylogeny():\n",
    "            break\n",
    "\n",
    "        t=tree\n",
    "        \n",
    "    elif type_ == 'newick':\n",
    "        t = Tree(filename, format=1)\n",
    "    else:\n",
    "        raise ValueError('Not a correct type.')\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct tree from distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTRUCT_PHYLO=False\n",
    "if CONSTRUCT_PHYLO:\n",
    "    ALL_dm_ldistance = load_dm(\n",
    "        OUTPUT_DIR + 'dm'+str(threshold)+'.csv', \n",
    "        upper_diag=False)\n",
    "    \n",
    "    distance_matrix_to_phylo_tree(\n",
    "        ALL_dm_ldistance, PHYLO_TREE_DIR + 'ldistance'+str(threshold)+'.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert phyloxml tree to newick tree to manipulate trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './ldistance6.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1407309/2919197443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m Phylo.convert(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mPHYLO_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ldistance'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.xml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'phyloxml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     PHYLO_DIR + 'ldistance'+str(threshold)+'.nhx','newick')\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ltree = load_tree(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Phylo/_io.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(in_file, in_format, out_file, out_format, parse_args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mparse_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Phylo/_io.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(trees, file, format, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_formats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Phylo/NewickIO.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(trees, handle, plain, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Phylo/NewickIO.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, handle, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;34m\"\"\"Write this instance's trees to a file handle.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtreestr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreestr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Phylo/NewickIO.py\u001b[0m in \u001b[0;36mto_strings\u001b[0;34m(self, confidence_as_branch_length, branch_length_only, plain, plain_newick, ladderize, max_confidence, format_confidence, format_branch_length)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# Convert each tree to a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mladderize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LEFT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RIGHT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# Nexus compatibility shim, kind of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/Phylo/_io.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(file, format, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_formats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"parse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.10/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/Bio/File.py\u001b[0m in \u001b[0;36mas_handle\u001b[0;34m(handleish, mode, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandleish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ldistance6.xml'"
     ]
    }
   ],
   "source": [
    "\n",
    "Phylo.convert(\n",
    "    PHYLO_DIR + 'ldistance'+str(threshold)+'.xml','phyloxml',\n",
    "    PHYLO_DIR + 'ldistance'+str(threshold)+'.nhx','newick')\n",
    "\n",
    "ltree = load_tree(\n",
    "    PHYLO_DIR + 'ldistance'+str(threshold)+'.nhx',\n",
    "    type_='newick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label nodes in tree to add other attributes like subtype risk etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandify(val,min=6.05,max=6.71,flag=False):\n",
    "    maptoten=int(np.ceil(((val-min)/(max-min))*10))\n",
    "    L=' '+u'\\u2580'*maptoten\n",
    "    \n",
    "    if flag:\n",
    "        L=L+u'\\u21DD'\n",
    "    return L\n",
    "\n",
    "def bandify(val,min=6,max=8,flag=False):\n",
    "    maptoten=int(np.ceil(((val-min)/(max-min))*10))\n",
    "    L=' '+u'\\u2501'*maptoten\n",
    "    if flag:\n",
    "        L=L+u'\\u2605'\n",
    "    return L\n",
    "\n",
    "def label_nodes(\n",
    "        tree, \n",
    "        recordinfo):\n",
    "    \"\"\"Label the nodes of the tree.\n",
    "    \n",
    "    We label nodes on whether:\n",
    "        it is covid19\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = copy.deepcopy(tree)\n",
    "    \n",
    "    for node in tree:\n",
    "        name = node.name      \n",
    "        node.subtype = recordinfo.seq_infos[name].subtype\n",
    "        node.erisk =recordinfo.seq_infos[name].erisk\n",
    "        node.id = recordinfo.seq_infos[name].name + bandify(recordinfo.seq_infos[name].erisk,min=threshold)\n",
    "        if VERBOSE:\n",
    "            print(node.name,node.subtype,node.id,node.erisk)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct labelled tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_tree=label_nodes(\n",
    "    ltree, ALLinfoHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to collapse similar leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_nodes(t):\n",
    "    # collapsed nodes are labeled, so you locate them and prune them\n",
    "    for n in t.search_nodes(collapsed=True):\n",
    "        for ch in n.get_children():\n",
    "            ch.detach()\n",
    "            \n",
    "            \n",
    "def mean(array):\n",
    "    return sum(array)/float(len(array))\n",
    "\n",
    "def cache_distances(tree):\n",
    "    ''' precalculate distances of all nodes to the root''' \n",
    "    node2rootdist = {tree:0}\n",
    "    for node in tree.iter_descendants('preorder'):\n",
    "        node2rootdist[node] = node.dist + node2rootdist[node.up]\n",
    "    return node2rootdist\n",
    "\n",
    "def closest_node(node, node2tips, root_distance):\n",
    "    \"\"\"Find the closest node.\"\"\"\n",
    "    \n",
    "    tips = []\n",
    "    distances = []\n",
    "    for tip in node2tips[node]:\n",
    "        distances.append(root_distance[tip]-root_distance[node])\n",
    "        tips.append(tip)\n",
    "        #     index = np.argmin([root_distance[tip]-root_distance[node] for tip in node2tips[node]])\n",
    "    index = np.argmin(distances)\n",
    "    return tips[index]\n",
    "\n",
    "def riskiest_node(node, node2tips):\n",
    "    \"\"\"Find the riskiest node.\"\"\"\n",
    "    \n",
    "    tips = []\n",
    "    risks = []\n",
    "    for tip in node2tips[node]:\n",
    "        risks.append(tip.erisk)\n",
    "        tips.append(tip)\n",
    "        #     index = np.argmin([root_distance[tip]-root_distance[node] for tip in node2tips[node]])\n",
    "    index = np.argmax(risks)\n",
    "    return tips[index]\n",
    "\n",
    "\n",
    "def all_collapsed(node, node2tips,AllrecordInfo):\n",
    "    \"\"\"Find all nodes in collapsed set.\"\"\"\n",
    "    \n",
    "    tips = []\n",
    "    for tip in node2tips[node]:\n",
    "        tips.append(AllrecordInfo.seq_infos[tip.name].name)\n",
    "    return tips\n",
    "\n",
    "def all_collapsed(node, node2tips,AllrecordInfo):\n",
    "    \"\"\"Find all nodes in collapsed set.\"\"\"\n",
    "    \n",
    "    tips = []\n",
    "    for tip in node2tips.get(node, []):  # use get() to avoid KeyError if node has no entries in node2tips\n",
    "        tips.append(AllrecordInfo.seq_infos[tip.name].name)\n",
    "        \n",
    "    # if no tips were found and the node is a leaf, append the node itself\n",
    "    if not tips and node.is_leaf():\n",
    "        tips.append(AllrecordInfo.seq_infos[node.name].name)\n",
    "\n",
    "    return tips\n",
    "\n",
    "\n",
    "def collapse(tree, min_dist,AllrecordInfo):\n",
    "    # cache the tip content of each node to reduce the number of times the tree is traversed\n",
    "    \n",
    "    tree = copy.deepcopy(tree)\n",
    "    \n",
    "    node2tips = tree.get_cached_content()\n",
    "    root_distance = cache_distances(tree)\n",
    "\n",
    "    for node in tree.get_descendants('preorder'):\n",
    "        IRAT=False\n",
    "        if not node.is_leaf():\n",
    "            avg_distance_to_tips = mean([root_distance[tip]-root_distance[node]\n",
    "                                         for tip in node2tips[node]])\n",
    "            if VERBOSE:\n",
    "                print(avg_distance_to_tips)\n",
    "            if avg_distance_to_tips <= min_dist:\n",
    "                # do whatever, ete support node annotation, deletion, labeling, etc.\n",
    "            \n",
    "                #closest_name = closest_node(node, node2tips, root_distance).name\n",
    "                \n",
    "                # find if this name is in IRATSEQ ie any IRAT seq is in teh collapsed nosdees\n",
    "                all_collapsed_nodes = all_collapsed(node, node2tips,AllrecordInfo)\n",
    "                #print(all_collapsed_nodes)\n",
    "                \n",
    "                \n",
    "                \n",
    "                for i in IRATSEQ:\n",
    "                    if i in all_collapsed_nodes:\n",
    "                        IRAT=True\n",
    "                        break\n",
    "                                       \n",
    "                closest_name = riskiest_node(node, node2tips).name\n",
    "                node.subtype = AllrecordInfo.seq_infos[closest_name].subtype\n",
    "                node.id = AllrecordInfo.seq_infos[closest_name].name + bandify(AllrecordInfo.seq_infos[closest_name].erisk,min=threshold,flag=IRAT) \n",
    "                node.name = '%s (%g)' %(closest_name,avg_distance_to_tips)\n",
    "                \n",
    "            \n",
    "                node.add_features(collapsed=True)\n",
    "\n",
    "                # set drawing attribute so they look collapsed when displayed with tree.show()\n",
    "                node.img_style['draw_descendants'] = False\n",
    "        else:\n",
    "            all_collapsed_nodes = all_collapsed(node, node2tips,AllrecordInfo)\n",
    "            for i in IRATSEQ:\n",
    "                if i in all_collapsed_nodes:\n",
    "                    IRAT=True\n",
    "                    break\n",
    "            node.id = AllrecordInfo.seq_infos[node.name].name + bandify(AllrecordInfo.seq_infos[node.name].erisk,min=threshold,flag=IRAT) \n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse leaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_collapsed=19\n",
    "ltree_collapsed = collapse(\n",
    "    labelled_tree, \n",
    "    min_dist=num_collapsed, \n",
    "    AllrecordInfo=ALLinfoHA)\n",
    "\n",
    "prune_nodes(ltree_collapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# code for actual rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLBAT='DarkRed'\n",
    "# COLRAT='SteelBlue'\n",
    "COLH3N2='Blue'\n",
    "COLH1N1='DarkRed'\n",
    "COLH7N9='DarkGreen'\n",
    "COLH9N2='#BD890F'\n",
    "COLDEF='BLACK'\n",
    "\n",
    "FS=50\n",
    "PW=10\n",
    "\n",
    "\n",
    "def nodeAttribConstruct(color, node):\n",
    "    N = AttrFace(\n",
    "        \"id\", fsize=FS, \n",
    "        text_prefix=\" \",penwidth=PW,ftype='Arial',\n",
    "        fgcolor=color,fstyle='bold')\n",
    "    faces.add_face_to_node(N, node, 1, position=\"branch-right\")\n",
    "    return N\n",
    "\n",
    "def layout(node):\n",
    "    if node.is_leaf():\n",
    "        if  node.subtype == 'H1N1':\n",
    "            N = nodeAttribConstruct(COLH1N1,node)\n",
    "        elif node.subtype == 'H3N2':\n",
    "            N = nodeAttribConstruct(COLH3N2,node)\n",
    "        elif node.subtype == 'H7N9':\n",
    "            N = nodeAttribConstruct(COLH7N9,node)\n",
    "        elif node.subtype == 'H9N2':\n",
    "            N = nodeAttribConstruct(COLH9N2,node)\n",
    "        else:\n",
    "            N = nodeAttribConstruct(COLDEF,node)\n",
    "            \n",
    "\n",
    "            \n",
    "def render_tree(tree, outfile):# all_seq_data, display_type='nearest_host'):\n",
    "    \"\"\"Render the tree inside the file to a circular \n",
    "    phylogenetic tree.\n",
    "    \n",
    "    NOTE: outfile should be in .pdf for best visuals\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    #tree = Tree(nwfile,format=1)\n",
    "\n",
    "    ts = TreeStyle()\n",
    "    ns = NodeStyle()\n",
    "    ts.show_leaf_name = False\n",
    "    #ts.rotation = 90\n",
    "    ts.mode = \"r\"\n",
    "    #ts.arc_start = -360 # 0 degrees = 3 o'clock\n",
    "    #ts.arc_span = 360\n",
    "    ts.scale=5\n",
    "    ts.show_scale=False\n",
    "    ts.branch_vertical_margin = .5 # 10 pixels between adjacent branches\n",
    "    # ts.show_branch_length=True\n",
    "    #ts.min_leaf_separation=10\n",
    "    #ts.optimal_scale_level='full'\n",
    "    #ts.branch_vertical_margin=0\n",
    "    \n",
    "    ns.hz_line_width=2\n",
    "    ns.vt_line_width=1\n",
    "    #ts.layout_fn = layout\n",
    "    ns[\"vt_line_width\"] = 16\n",
    "    ns[\"hz_line_width\"] = 16\n",
    "    #     ns['fsize'] = 20\n",
    "    for n in tree.traverse():\n",
    "        n.set_style(ns)\n",
    "        \n",
    "    #all_accessions = all_seq_data['accessions'].values\n",
    "    for n in tree:\n",
    "        ts.layout_fn = layout\n",
    "\n",
    "        \n",
    "    tree.set_style(ns)\n",
    "    tree.set_style(ts)\n",
    "    \n",
    "    ax=tree.render(\n",
    "        outfile, \n",
    "        dpi=300, \n",
    "        w=500,\n",
    "        tree_style=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=5\n",
    "medrisknames=[]\n",
    "for node in ltree_collapsed:\n",
    "    if ALLinfoHA.seq_infos[node.name.split()[0]].erisk > r:\n",
    "        medrisknames=np.append(medrisknames,\n",
    "                                ALLinfoHA.seq_infos[node.name.split()[0]].name)\n",
    "medrisknames=df[df.id.isin(medrisknames)][['id','subtype',\n",
    "                                               'ha_accession',\n",
    "                                               'na_accession',\n",
    "                                               'impact_risk',\n",
    "                                               'emergence_risk']].sort_values('emergence_risk',\n",
    "                                                                                         ascending=False)\n",
    "medrisknames.to_csv('allriskystrains_collapsed.csv',index=None)\n",
    "medrisknames[medrisknames.id.str.contains('Ohio')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render phylogenetic trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=render_tree(\n",
    "    labelled_tree,\n",
    "    './riskyphylo'+str(threshold)+'irat.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=render_tree(\n",
    "    ltree_collapsed,\n",
    "    'riskyphylo'+str(threshold)+'_collapsed_'+str(num_collapsed)+'irat.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltree_collapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find high risk strains which are on distinct branches on phylogenetic tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=6.057\n",
    "r=6.25\n",
    "highrisknames=[]\n",
    "for node in ltree_collapsed:\n",
    "    if ALLinfoHA.seq_infos[node.name.split()[0]].erisk > r:\n",
    "        highrisknames=np.append(highrisknames,\n",
    "                                ALLinfoHA.seq_infos[node.name.split()[0]].name)\n",
    "highrisknamesdf=df[df.id.isin(highrisknames)][['id','subtype',\n",
    "                                               'ha_accession',\n",
    "                                               'na_accession',\n",
    "                                               'impact_risk',\n",
    "                                               'emergence_risk']].sort_values('emergence_risk',\n",
    "                                                                                         ascending=False)\n",
    "highrisknamesdf = highrisknamesdf.rename(columns={'id':'strain',\n",
    "                                                  'ha_accession':'HA accession',\n",
    "                                                  'na_accession':'NA accession',\n",
    "                                                  'impact_risk':'predicted IRAT impact',\n",
    "                                                  'emergence_risk':'predicted IRAT emergence'}).set_index('strain')\n",
    "highrisknamesdf#.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLDICT={'H1N1':'Red3!20','H3N2':'Blue1!30','H7N9':'Green3!50','H9N2':'DarkOrange!40'}\n",
    "def rowcolor(row):\n",
    "    return '\\\\rowcolor{' + COLDICT[row.subtype]+'}' + row['strain']\n",
    "\n",
    "highrisknamesdf1 = highrisknamesdf.reset_index(drop=False)\n",
    "highrisknamesdf1['strain']=highrisknamesdf1.apply(rowcolor,axis=1)\n",
    "highrisknamesdf1=highrisknamesdf1.set_index('strain')\n",
    "highrisknamesdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zedstat.textable import textable\n",
    "textable(highrisknamesdf1,#pd.concat([highrisknamesdf1.head(30),highrisknamesdf1.tail(10)]),\n",
    "         tabname='highrisk35.tex',\n",
    "         FORMAT='%1.4f',INDEX=True,\n",
    "         TABFORMAT='L{1.95in}|L{.25in}|L{.60in}|L{.6in}|C{1in}|C{1in}',LNTERM='\\\\\\\\\\n')\n",
    "! cat highrisk35.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "Subtype={'H1N1':0,'H3N2':0,'H7N9':0,'H9N2':0}\n",
    "MaxriskStrain={'H1N1':None,'H3N2':None,'H7N9':None,'H9N2':None}\n",
    "Subtype_strat={ 6.0:{'H1N1':0,'H3N2':0,'H7N9':0,'H9N2':0},  6.2:{'H1N1':0,'H3N2':0,'H7N9':0,'H9N2':0},              6.3:{'H1N1':0,'H3N2':0,'H7N9':0,'H9N2':0},\n",
    "               6.4:{'H1N1':0,'H3N2':0,'H7N9':0,'H9N2':0},6.5:{'H1N1':0,'H3N2':0,'H7N9':0,'H9N2':0}}\n",
    "for node in ltree_collapsed:\n",
    "    Subtype[node.subtype]=Subtype[node.subtype]+1\n",
    "    if MaxriskStrain[node.subtype] is None:\n",
    "        MaxriskStrain[node.subtype]=(ALLinfoHA.seq_infos[node.name.split()[0]].name,\n",
    "                                     ALLinfoHA.seq_infos[node.name.split()[0]].erisk)\n",
    "    else:\n",
    "        if MaxriskStrain[node.subtype][1]<ALLinfoHA.seq_infos[node.name.split()[0]].erisk:\n",
    "            MaxriskStrain[node.subtype]=(ALLinfoHA.seq_infos[node.name.split()[0]].name,\n",
    "                                         ALLinfoHA.seq_infos[node.name.split()[0]].erisk)\n",
    "    for r in [6.0, 6.2,6.3,6.4,6.5]:\n",
    "        if r<ALLinfoHA.seq_infos[node.name.split()[0]].erisk:\n",
    "            #print(node.subtype,ALLinfoHA.seq_infos[node.name.split()[0]].erisk)\n",
    "            Subtype_strat[r][node.subtype]=Subtype_strat[r][node.subtype]+1\n",
    "        \n",
    "    count=count+1\n",
    "maxriskdf = pd.DataFrame(MaxriskStrain)\n",
    "Subtype_strat_df = pd.DataFrame(Subtype_strat)\n",
    "display(maxriskdf)\n",
    "display(Subtype_strat_df)\n",
    "print(count)\n",
    "df[df.id.isin(maxriskdf.iloc[0,:].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zedstat.textable import textable\n",
    "Subtype_strat_df.columns=[str(x) for x in Subtype_strat_df.columns]\n",
    "Subtype_strat_df.index.name='subtype'\n",
    "Subtype_strat_df=Subtype_strat_df.reset_index()\n",
    "textable(Subtype_strat_df,#pd.concat([highrisknamesdf1.head(30),highrisknamesdf1.tail(10)]),\n",
    "         tabname='riskycount.tex',\n",
    "         FORMAT='%1.4f',INDEX=False,\n",
    "         TABFORMAT='L{.75in}|L{.25in}|L{.60in}|L{.6in}|L{.6in}|L{.6in}',LNTERM='\\\\\\\\\\n')\n",
    "! cat riskycount.tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta(seqs, fasta_file, wrap=80):\n",
    "    \"\"\"Write sequences to a fasta file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seqs : dict[seq_id] -> seq\n",
    "        Sequences indexed by sequence id.\n",
    "    fasta_file : str\n",
    "        Path to write the sequences to.\n",
    "    wrap: int\n",
    "        Number of AA/NT before the line is wrapped.\n",
    "    \"\"\"\n",
    "    with open(fasta_file, 'w') as f:\n",
    "        for gid, gseq in seqs.items():\n",
    "            f.write('>{}\\n'.format(gid))\n",
    "            for i in range(0, len(gseq), wrap):\n",
    "                f.write('{}\\n'.format(gseq[i:i + wrap])) \n",
    "\n",
    "#for i,name in zip(df[df.id.isin(maxriskdf.iloc[0,:].values)].ha.values,\n",
    "#                 df[df.id.isin(maxriskdf.iloc[0,:].values)].ha_accession.values):\n",
    "#    write_fasta({name:i},name+'.fasta')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list=['A/swine/North_Carolina/A02479173/2020','A/swine/Tennessee/A02524414/2022']\n",
    "RFILES={name+'('+subtype+')':i for i,name,subtype in zip(df[df.id.isin(id_list)].ha.values,\n",
    "                 df[df.id.isin(id_list)].ha_accession.values,\n",
    "                 df[df.id.isin(id_list)].subtype.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTANAME='risky4.fasta'\n",
    "FASTAFILES={name+'('+subtype+')':i for i,name,subtype in zip(df[df.id.isin(maxriskdf.iloc[0,:].values)].ha.values,\n",
    "                 df[df.id.isin(maxriskdf.iloc[0,:].values)].ha_accession.values,\n",
    "                 df[df.id.isin(maxriskdf.iloc[0,:].values)].subtype.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dom_seq='../../../qnet_predictions/results/dominant_sequences_2021_2022.csv'\n",
    "domf=pd.read_csv(human_dom_seq,index_col=0)\n",
    "domf=domf.assign(subtype=[x.split('_')[1].upper() for x in domf.index.values],protein=[x.split('_')[2].upper() for x in domf.index.values])\n",
    "domf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dom_seq='../../../../paper_data/enet_predictions/results/dominant_sequences/north_h1n1_ha/north_h1n1_ha_21_22.csv'\n",
    "domf=pd.read_csv(human_dom_seq,index_col=0)\n",
    "domf=domf.assign(subtype='H1N1',protein='HA')\n",
    "domf=domf[domf.protein=='HA'].head(2)\n",
    "DOMFILES={'DOM_HUMAN'+'('+subtype+')':i for i,name,subtype in zip(domf.sequence.values,\n",
    "                 domf.id.values,\n",
    "                 domf.subtype.values)}\n",
    "DOMFILES.update(FASTAFILES)\n",
    "DOMFILES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "human_dom_seq='../../../../paper_data/enet_predictions/results/dominant_sequences/north_h1n1_ha/north_h1n1_ha_21_22.csv'\n",
    "#human_dom_seq='../../../qnet_predictions/results/dominant_sequences_2021_2022.csv'\n",
    "domf=pd.read_csv(human_dom_seq,index_col=0)\n",
    "domf=domf.assign(subtype=[x.split('_')[1].upper() for x in domf.index.values],protein=[x.split('_')[2].upper() for x in domf.index.values])\n",
    "domf=domf[domf.protein=='HA'].head(2)\n",
    "DOMFILES={'DOM_HUMAN'+'('+subtype+')':i for i,name,subtype in zip(domf.sequence.values,\n",
    "                 domf.name.values,\n",
    "                 domf.subtype.values)}\n",
    "DOMFILES.update(FASTAFILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMFILES.update(RFILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMFILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_fasta(DOMFILES,FASTANAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Blast import NCBIWWW\n",
    "my_query = SeqIO.read(\"EPI1818121.fasta\", format=\"fasta\")\n",
    "result_handle = NCBIWWW.qblast(\"blastp\", \"nr\", my_query.seq)\n",
    "blast_result = open(\"my_blast.xml\", \"w\")\n",
    "blast_result.write(result_handle.read())\n",
    "blast_result.close()\n",
    "result_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_blast(resultfile): #takes in the BLAST result, outputs list that can be made into csv\n",
    "    from Bio.Blast import NCBIXML\n",
    "    result_handle = open(resultfile)\n",
    "    blast_records = NCBIXML.parse(result_handle)\n",
    "    csv_list = []\n",
    "    \n",
    "    header = [  'accession','Query',\n",
    "                'Name', 'Length', 'Score', 'Expect',\n",
    "                'QueryStart', 'QueryEnd',\n",
    "                'SubjectStart', 'SubjectEnd','pct'\n",
    "            ]\n",
    "    \n",
    "    #csv_list.append(header)\n",
    "    count = 0\n",
    "    for blast_record in blast_records:\n",
    "        '''help(blast_record.alignments[0].hsps[0])''' # these give help info for the parts \n",
    "        '''help(blast_record.alignments[0])        '''\n",
    "        count +=1\n",
    "        \n",
    "        query = blast_record.query\n",
    "        for alignment in blast_record.alignments:\n",
    "\n",
    "            name = alignment.title\n",
    "            length = alignment.length\n",
    "    \n",
    "            hsp = alignment.hsps[0] # I don't know if we will ever have more than one, so might as well take the first one.\n",
    "            score = hsp.score\n",
    "            expect = hsp.expect\n",
    "            querystart = hsp.query_start\n",
    "            queryend = hsp.query_end\n",
    "            subjectstart = hsp.sbjct_start\n",
    "            subjectend = hsp.sbjct_end\n",
    "            pct=hsp.positives/hsp.align_length\n",
    "            accession=alignment.accession\n",
    "            row = [accession,query,name,length,score,expect,querystart,queryend,subjectstart,subjectend,pct]\n",
    "            csv_list.append(row)\n",
    "            \n",
    "    result_handle.close()\n",
    "    return pd.DataFrame(csv_list,columns=header)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_list=parse_blast('my_blast.xml')\n",
    "csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = \"ishanu@uchicago.edu\"     # Always tell NCBI who you are\n",
    "handle = Entrez.esearch(db=\"protein\", term=\"QJT24340\")\n",
    "record = Entrez.read(handle)\n",
    "id=record[\"IdList\"]\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = Entrez.efetch(db=\"protein\", rettype=\"gb\", retmode=\"text\", id=\"1834373412\")\n",
    "#record = SeqIO.read(handle, \"genbank\")\n",
    "handle.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H9N2: minks are probable mixing vessels\n",
    "https://www.ncbi.nlm.nih.gov/nuccore/ON870694.1\n",
    " https://www.tandfonline.com/doi/full/10.1080/22221751.2021.1899058\n",
    " Pandemic influenza, typically caused by the reassortment of human and avian influenza viruses, can result in severe or fatal infections in humans. Timely identification of potential pandemic viruses must be a priority in influenza virus surveillance. However, the range of host species responsible for the generation of novel pandemic influenza viruses remains unclear. In this study, we conducted serological surveys for avian and human influenza virus infections in farmed mink and determined the susceptibility of mink to prevailing avian and human virus subtypes. The results showed that farmed mink were commonly infected with human (H3N2 and H1N1/pdm) and avian (H7N9, H5N6, and H9N2) influenza A viruses. Correlational analysis indicated that transmission of human influenza viruses occurred from humans to mink, and that feed source was a probable route of avian influenza virus transmission to farmed mink. Animal experiments showed that mink were susceptible and permissive to circulating avian and human influenza viruses, and that human influenza viruses (H3N2 and H1N1/pdm), but not avian viruses, were capable of aerosol transmission among mink. These results indicate that farmed mink could be highly permissive “mixing vessels” for the reassortment of circulating human and avian influenza viruses. \n",
    " \n",
    " H7N9\n",
    " https://www.bmj.com/content/347/bmj.f4752?tab=responses\n",
    " HH transmission has been suspected\n",
    " Asian lineage H7N9 virus is rated by the Influenza Risk Assessment Tool as having the greatest potential to cause a pandemic, as well as potentially posing the greatest risk to severely impact public health if it were to achieve sustained human-to-human transmission.\n",
    " \n",
    " H3N2 swine\n",
    " https://www.cdc.gov/flu/swineflu/spotlights/first-human-infection-2022.htm\n",
    " Human infection detected in US this Aug\n",
    " \n",
    " H1N1\n",
    " Submitted (03-NOV-2020) USDA Swine Surveillance, USDA Swine\n",
    "            Surveillance, 1920 Dayton, Ames, IA 50010, USA\n",
    "COMMENT     Method: conceptual translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=itertools.repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
